{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8gdIuFzmGg02"
   },
   "source": [
    "### Mods for Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prevent Inactivity Timeout**\n",
    "\n",
    "`Ctrl+Shift+I` to open Dev Tools, then paste the below into Console and press Enter after starting training loop.\n",
    "```javascript\n",
    "setInterval(() => {\n",
    "    console.log(\"Simulating activity...\");\n",
    "\n",
    "    // scroll up and down by 100 pixels\n",
    "    window.scrollBy(0, -100);\n",
    "    setTimeout(() => {\n",
    "        window.scrollBy(0, 100);\n",
    "    }, 500);\n",
    "\n",
    "    // click the connectivity button programmatically\n",
    "    const button = document.querySelector('colab-connect-button');\n",
    "    if (button) {\n",
    "        button.click();\n",
    "        console.log(\"Connectivity button clicked!\");\n",
    "    } else {\n",
    "        console.error(\"Connectivity button not found!\");\n",
    "    }\n",
    "}, 120000); // run every 2 minutes\n",
    "```\n",
    "\n",
    "Kill script by running the below in a new cell.\n",
    "```javascript\n",
    "for (let i = 0; i < 100000; i++) {\n",
    "    clearInterval(i);\n",
    "    clearTimeout(i);\n",
    "}\n",
    "console.log(\"All intervals cleared!\");\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JPu7n7EnGiMA",
    "outputId": "c56e96e5-5aab-4636-ff3c-42d54d2f7fdd"
   },
   "outputs": [],
   "source": [
    "!pip install triton # required by TorchInductor (backend compiler in PyTorch)\n",
    "!pip install bitsandbytes # needed in model_loader for quantization\n",
    "!pip install cloud-tpu-client # for tpu connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3KrPvsWPGlqU",
    "outputId": "a07c4801-d1b0-4578-876e-66718e2281e8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import drive\n",
    "\n",
    "colab = True # global indicator of colab run\n",
    "drive.flush_and_unmount() # unmount existing session\n",
    "# mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "# set working dir to finetuning dir\n",
    "os.chdir(os.path.join(\"/content/drive/My Drive\", \"agreemate\", \"finetuning\"))\n",
    "\n",
    "print(\"Working directory:\", os.getcwd())\n",
    "print(\"Files in directory:\", os.listdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gBlJLD05GdYF"
   },
   "source": [
    "# AgreeMate Model Testing Notebook\n",
    "\n",
    "Mini notebook for testing baseline and finetuned models on CraigslistBargains data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dS0E_dnmGdYG"
   },
   "outputs": [],
   "source": [
    "import os, sys, gc, logging, re, torch\n",
    "from typing import Optional, Tuple, List, Dict, Any\n",
    "from dataclasses import dataclass\n",
    "from logging.handlers import RotatingFileHandler\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "from agreemate.baseline.utils.data_loader import DataLoader\n",
    "from model_loader import ModelLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2PFvqr98GdYH"
   },
   "source": [
    "Setup Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q5nZj3zdGdYH"
   },
   "outputs": [],
   "source": [
    "class TqdmCompatibleStreamHandler(logging.StreamHandler):\n",
    "    \"\"\"\n",
    "    Custom logging handler that avoids interfering with tqdm progress bars.\n",
    "    \"\"\"\n",
    "    def emit(self, record):\n",
    "        try:\n",
    "            msg = self.format(record)\n",
    "            tqdm.write(msg) # use tqdm's write method for compatibility\n",
    "            self.flush()\n",
    "        except Exception:\n",
    "            self.handleError(record)\n",
    "\n",
    "# file handler for logging everything to a file (limit size to 5MB)\n",
    "file_handler = RotatingFileHandler(\n",
    "    os.path.join(os.getcwd(), \"tests.log\"),\n",
    "    maxBytes=10**6, backupCount=5\n",
    ")\n",
    "file_handler.setLevel(logging.DEBUG)\n",
    "\n",
    "# console handler for logging to notebook's output\n",
    "console_handler = TqdmCompatibleStreamHandler()\n",
    "console_handler.setLevel(logging.INFO)\n",
    "\n",
    "# set up the logging configuration\n",
    "logging.basicConfig(\n",
    "    handlers=[file_handler, console_handler],\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    level=logging.DEBUG,\n",
    "    force=True, # force logging reconfig\n",
    ")\n",
    "\n",
    "# create logger\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# suppress overly verbose logs in notebook output\n",
    "logging.getLogger(\"transformers\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"datasets\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"torch\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"accelerate\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v7BL4jaEGdYH"
   },
   "source": [
    "## Scenario Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JCFZKcvUGdYH"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Scenario:\n",
    "    \"\"\"Formats raw scenario data into structured buyer/seller knowledge bases.\"\"\"\n",
    "    def __init__(self, scenario_dict):\n",
    "        self.title = scenario_dict['item']['title']\n",
    "        self.description = scenario_dict['item']['description']\n",
    "        self.category = scenario_dict['category']\n",
    "        self.listing_price = scenario_dict['item']['list_price']\n",
    "        self.buyer_target = scenario_dict['buyer']['target_price']\n",
    "        self.seller_target = scenario_dict['seller']['target_price']\n",
    "\n",
    "        self.buyer_kb = {\n",
    "            \"role\": \"buyer\",\n",
    "            \"title\": self.title,\n",
    "            \"description\": self.description,\n",
    "            \"category\": self.category,\n",
    "            \"listing_price\": self.listing_price,\n",
    "            \"target_price\": self.buyer_target,\n",
    "            \"relative_price\": scenario_dict['buyer']['relative_price']\n",
    "        }\n",
    "\n",
    "        self.seller_kb = {\n",
    "            \"role\": \"seller\",\n",
    "            \"title\": self.title,\n",
    "            \"description\": self.description,\n",
    "            \"category\": self.category,\n",
    "            \"listing_price\": self.listing_price,\n",
    "            \"target_price\": self.seller_target,\n",
    "            \"price_delta_pct\": scenario_dict['seller']['price_delta_pct']\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xcK6QF7JGdYI"
   },
   "outputs": [],
   "source": [
    "class Dialogue:\n",
    "    \"\"\"Single dialogue turn in negotiation.\"\"\"\n",
    "    def __init__(self, utterance, action, price=None, agent=\"unknown\"):\n",
    "        self.utterance = utterance\n",
    "        self.action = action\n",
    "        self.price = price if action in {\"price\", \"counter\", \"emphasize\"} else None\n",
    "        self.agent = agent\n",
    "        self.current_price = None\n",
    "\n",
    "    def extract_price(self):\n",
    "        \"\"\"Extracts price from utterance if present.\"\"\"\n",
    "        matched_price = re.search(r'\\$\\s?(\\d+(\\.\\d+)?)', self.utterance)\n",
    "        if matched_price:\n",
    "            self.price = float(matched_price.group(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ycBkVMG9GdYI"
   },
   "outputs": [],
   "source": [
    "class Negotiation:\n",
    "    \"\"\"Manages complete negotiation dialogue between agents.\"\"\"\n",
    "    def __init__(self, scenario, buyer, seller, parser_model):\n",
    "        self.scenario = scenario\n",
    "        self.buyer = buyer\n",
    "        self.seller = seller\n",
    "        self.parser_model = parser_model\n",
    "        self.conversation_history = []\n",
    "        self.prepared_conversation_history = \"\"\n",
    "\n",
    "        self.accepted = False\n",
    "        self.accepted_price = None\n",
    "\n",
    "    def add_to_history(self, dialogue):\n",
    "        \"\"\"Adds turn to conversation history.\"\"\"\n",
    "        self.conversation_history.append(dialogue)\n",
    "        self.prepared_conversation_history = \"\\n\".join(\n",
    "            f\"{d.agent.capitalize()}: {d.utterance}\" for d in self.conversation_history\n",
    "        )\n",
    "\n",
    "    def run(self, max_turns=10):\n",
    "        \"\"\"Executes negotiation dialogue and return results.\"\"\"\n",
    "        logger.info(f\"\\nNegotiation for: {self.scenario.title}\")\n",
    "        logger.info(f\"List Price: ${self.scenario.listing_price}\")\n",
    "        logger.info(\"=\"*80)\n",
    "\n",
    "        turn = 0\n",
    "        current_price = None\n",
    "\n",
    "        while turn < max_turns:\n",
    "            agent = self.buyer if turn % 2 == 0 else self.seller\n",
    "            dialogue = agent(self.prepared_conversation_history,\n",
    "                                      (max_turns - turn) // 2, self.parser_model)\n",
    "\n",
    "            logger.info(f\"{dialogue.agent.capitalize()}: {dialogue.utterance}\")\n",
    "            self.add_to_history(dialogue)\n",
    "            turn += 1\n",
    "\n",
    "            if dialogue.price:\n",
    "                current_price = dialogue.price\n",
    "            if dialogue.action == \"accept\":\n",
    "                self.accepted = True\n",
    "                self.accepted_price = current_price\n",
    "                break\n",
    "            if dialogue.action == \"reject\":\n",
    "                self.accepted = False\n",
    "                break\n",
    "\n",
    "        logger.info(\"~\"*80)\n",
    "        logger.info(f\"Outcome: {'Accepted' if self.accepted else 'Rejected'}\")\n",
    "        if self.accepted:\n",
    "            logger.info(f\"Final Price: ${self.accepted_price}\")\n",
    "        logger.info(\"=\"*80 + \"\\n\")\n",
    "\n",
    "        return {\n",
    "            \"accepted\": self.accepted,\n",
    "            \"final_price\": self.accepted_price,\n",
    "            \"num_turns\": len(self.conversation_history),\n",
    "            \"listing_price\": self.scenario.listing_price,\n",
    "            \"buyer_target\": self.scenario.buyer_target,\n",
    "            \"seller_target\": self.scenario.seller_target\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWfvSB73GdYI"
   },
   "source": [
    "## Prompting and Response Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SlZ6EgL6GdYI"
   },
   "source": [
    "Define system prompt and role-specific instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eAh9BB0_GdYI"
   },
   "outputs": [],
   "source": [
    "def get_buyer_instructions(kb: dict) -> str:\n",
    "    \"\"\"Returns buyer-specific instructions.\"\"\"\n",
    "    return f\"\"\"\n",
    "    ### Your Role as the Buyer:\n",
    "    - Goal: Negotiate as close to your target price (${kb['target_price']}) as possible.\n",
    "    - Strategy:\n",
    "        1. Begin with offers below your target price but reasonable enough to engage the seller.\n",
    "        2. Highlight flaws or alternatives to justify lower offers.\n",
    "        3. Avoid exceeding your target price, even if pressured.\n",
    "    - Be respectful but firm. Do not overcommit.\n",
    "\n",
    "    ### Example Buyer Responses:\n",
    "    1. Thought: I believe the item is worth $150 based on its flaws.\n",
    "       Action: offer\n",
    "       Utterance: I can offer $150 considering the wear and tear.\n",
    "    2. Thought: The seller's offer aligns with my target price.\n",
    "       Action: accept\n",
    "       Utterance: $200 works for me. Let's finalize this deal.\n",
    "    \"\"\"\n",
    "\n",
    "def get_seller_instructions(kb: dict) -> str:\n",
    "    \"\"\"Returns seller-specific instructions.\"\"\"\n",
    "    return f\"\"\"\n",
    "    ### Your Role as the Seller:\n",
    "    - Goal: Maximize the final price while keeping the buyer engaged.\n",
    "    - Strategy:\n",
    "        1. Anchor the negotiation around the listing price (${kb['listing_price']}).\n",
    "        2. Justify your price by emphasizing the item's quality or rarity.\n",
    "        3. Counter offers strategically, staying above your target price (${kb['target_price']}).\n",
    "\n",
    "    ### Example Seller Responses:\n",
    "    1. Thought: The buyer's offer is too low, but I want to keep negotiating.\n",
    "       Action: counter\n",
    "       Utterance: I can't do $150, but I can offer $180 given the item's excellent condition.\n",
    "    2. Thought: This is my lowest acceptable price; I'm ready to conclude.\n",
    "       Action: emphasize\n",
    "       Utterance: This item is priced competitively at $200 due to its rarity.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "def get_base_prompt(kb: dict, conversation_history: str, turns_left: int) -> str:\n",
    "    \"\"\"Generates a base prompt with common agent context and instructions.\"\"\"\n",
    "    return f\"\"\"\n",
    "    ### Negotiation Scenario:\n",
    "    - Item: {kb['title']}\n",
    "    - Description: {kb['description']}\n",
    "    - List Price: ${kb['listing_price']}\n",
    "    - Your Role: {kb['role'].capitalize()}\n",
    "    - Turns Left to Conclude: {turns_left}\n",
    "\n",
    "    ### Previous Conversation:\n",
    "    {conversation_history}\n",
    "\n",
    "    ### Response Instructions:\n",
    "    1. Consider the negotiation context and role-specific goals.\n",
    "    2. Think aloud about your reasoning before deciding your action.\n",
    "    3. Respond strictly in the following structured format:\n",
    "    ---\n",
    "    Thought: <your reasoning>\n",
    "    Action: <offer|counter|accept|reject>\n",
    "    Utterance: <your message>\n",
    "    (Optional) Price: $<price>\n",
    "    ---\n",
    "    Respond below:\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "def get_parser_prompt(response: str) -> str:\n",
    "    \"\"\"Generates a parsing prompt for the base model to structure an agent's response.\"\"\"\n",
    "    return f\"\"\"\n",
    "    ### Agent's Response (Free-form):\n",
    "    ---\n",
    "    {response}\n",
    "    ---\n",
    "\n",
    "    ### Parsing Instructions:\n",
    "    - Extract and structure the response using the following format:\n",
    "    ---\n",
    "    Action: <offer|counter|accept|reject>\n",
    "    Utterance: <the agent's message>\n",
    "    (Optional) Price: $<price>\n",
    "    ---\n",
    "    Respond in this exact format without any additional information.\n",
    "\n",
    "    Structured Output:\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gnuGIx-PGdYI"
   },
   "source": [
    "## Agent Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fh8t_1_FcTJ1"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Configuration for a model including path and type information.\"\"\"\n",
    "    name: str\n",
    "    model_type: str\n",
    "    path: str\n",
    "    model: Optional[object] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7xTg3t4UGdYI"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"Base agent class for negotiation participants.\"\"\"\n",
    "    def __init__(self, config: ModelConfig, role: str, kb: dict, tokenizer: object):\n",
    "        self.role = role\n",
    "        self.config = config\n",
    "        self.kb = kb\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    #! PROMPTING\n",
    "    def truncate_conversation(self, conversation_history: str, max_response_tokens: int = 100) -> str:\n",
    "        \"\"\"Truncates conversation history dynamically to fit within model token limits.\"\"\"\n",
    "        # get model's max token limit\n",
    "        max_tokens = getattr(self.config.model.config, \"max_position_embeddings\", 8192)\n",
    "\n",
    "        # tokenize base prompt and role instructions to calculate their size\n",
    "        base_prompt_tokens = len(self.tokenizer.encode(get_base_prompt(self.kb, \"\", 0), add_special_tokens=False))\n",
    "        role_instructions_tokens = len(self.tokenizer.encode(\n",
    "            get_buyer_instructions(self.kb) if self.role == \"buyer\" else get_seller_instructions(self.kb),\n",
    "            add_special_tokens=False\n",
    "        ))\n",
    "\n",
    "        # calculate available space for conversation history\n",
    "        reserved_tokens = 5 # BOS + EOS + any other special tokens (safe margin)\n",
    "        available_tokens = max_tokens - reserved_tokens - base_prompt_tokens - role_instructions_tokens - max_response_tokens\n",
    "        if available_tokens <= 0:\n",
    "            raise ValueError(\"Prompt components exceed the model's token limit!\")\n",
    "\n",
    "        conversation_tokens = self.tokenizer.encode(conversation_history, add_special_tokens=False) # tokenize history\n",
    "\n",
    "        if len(conversation_tokens) > available_tokens: # truncate conversation history if necessary and return\n",
    "            logger.info(\"Truncating conversation history to fit within token limits\")\n",
    "            logger.info(f\"Available tokens: {available_tokens} < {len(conversation_tokens)} (history length)\")\n",
    "            return self.tokenizer.decode(conversation_tokens[-available_tokens:], skip_special_tokens=True)\n",
    "\n",
    "        return conversation_history # return original history if it fits\n",
    "\n",
    "    def get_prompt(self, conversation_history: str, turns_left: int) -> str:\n",
    "        \"\"\"Stitches together role-specific prompt.\"\"\"\n",
    "        truncated_history = self.truncate_conversation(conversation_history) # truncate history if needed\n",
    "\n",
    "        # get base prompt and role-specific instructions\n",
    "        base_prompt = get_base_prompt(self.kb, truncated_history, turns_left)\n",
    "        role_instructions = (get_buyer_instructions if self.role == \"buyer\"\n",
    "                                                    else get_seller_instructions)(self.kb)\n",
    "\n",
    "        # combine role-specific instructions with base prompt\n",
    "        return f\"{role_instructions}\\n{base_prompt}\"\n",
    "\n",
    "\n",
    "    #! RESPONSE GENERATION AND PARSING\n",
    "    def generate_response(self, prompt: str, max_tokens: int = 150) -> str:\n",
    "        \"\"\"Generates response to prompt using the model.\"\"\"\n",
    "        inputs = self.tokenizer(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        ).to(self.config.model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.config.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_tokens,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                pad_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        response = self.tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:],\n",
    "                                                          skip_special_tokens=True)\n",
    "        return response\n",
    "\n",
    "    def parse_response(self, raw_response: str, parser_model) -> tuple:\n",
    "        \"\"\"Parse raw agent response into structured components (action, utterance, price) using a parser model.\"\"\"\n",
    "        logger.info(f\"Parsing Raw Response:\\n{raw_response}\\n\")\n",
    "\n",
    "        # prepare for parsing\n",
    "        parser_prompt = get_parser_prompt(raw_response)\n",
    "\n",
    "        # generate a structured response using the base model\n",
    "        inputs = self.tokenizer(\n",
    "            parser_prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        ).to(parser_model.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = parser_model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=20,\n",
    "                pad_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "        parsed_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        logger.info(f\"Parsed Response:\\n{parsed_response}\\n\")\n",
    "\n",
    "        # extract components directly from the structured response\n",
    "        components = {\"action\": None, \"utterance\": None, \"price\": None}\n",
    "        for line in parsed_response.split(\"\\n\"):\n",
    "            line = line.strip().lower()\n",
    "            if not line:\n",
    "                continue\n",
    "            for key in components:\n",
    "                if line.startswith(f\"{key}:\"):\n",
    "                    value = line.split(\":\", 1)[1].strip()\n",
    "                    if key == \"price\":\n",
    "                        try:\n",
    "                            value = float(value.replace(\"$\", \"\"))\n",
    "                        except ValueError:\n",
    "                            logger.warning(f\"Invalid price format: {value}\")\n",
    "                            value = None\n",
    "                    components[key] = value\n",
    "                    break\n",
    "\n",
    "        if not components[\"action\"] or not components[\"utterance\"]:\n",
    "            raise ValueError(f\"Invalid response format: {parsed_response}\")\n",
    "        return (components[\"utterance\"], components[\"action\"], components[\"price\"])\n",
    "\n",
    "\n",
    "    #! TURN\n",
    "    def __call__(self, conversation_history: str, turns_left: int, parser_model) -> Dialogue:\n",
    "        \"\"\"Generate and parse agent response.\"\"\"\n",
    "        # generate agent response\n",
    "        prompt = self.get_prompt(conversation_history, turns_left)\n",
    "        raw_response = self.generate_response(prompt)\n",
    "\n",
    "        # parse into structured components\n",
    "        utterance, action, price = self.parse_response(raw_response, parser_model)\n",
    "        logger.info(f\"Parsed Response - Action: {action}, Utterance: {utterance}, Price: {price if price else 'None'}\")\n",
    "\n",
    "        return Dialogue(utterance, action, price, self.role)\n",
    "\n",
    "\n",
    "class Buyer(Agent):\n",
    "    \"\"\"Buyer Agent.\"\"\"\n",
    "    def __init__(self, config: ModelConfig, kb: dict, tokenizer: object):\n",
    "        super().__init__(config, \"buyer\", kb, tokenizer)\n",
    "\n",
    "class Seller(Agent):\n",
    "    \"\"\"Seller Agent.\"\"\"\n",
    "    def __init__(self, config: ModelConfig, kb: dict, tokenizer: object):\n",
    "        super().__init__(config, \"seller\", kb, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D9FUg7MJGdYJ"
   },
   "source": [
    "## Run Test Negotiations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EpV_Py_Ee-wP"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TestResult:\n",
    "    \"\"\"Structured result from a negotiation test scenario.\"\"\"\n",
    "    buyer_model: str\n",
    "    seller_model: str\n",
    "    accepted: bool\n",
    "    num_turns: int\n",
    "    final_price: Optional[float]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WPSZ0ttwGdYJ"
   },
   "source": [
    "### Define Testing Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelManager:\n",
    "    \"\"\"\n",
    "    Manages model loading/unloading efficiently to minimize VRAM usage.\n",
    "    Ensures no more than 2 agent models are loaded at any time.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_configs: Dict[str, ModelConfig], model_loader: ModelLoader):\n",
    "        self.model_configs = model_configs\n",
    "        self.model_loader = model_loader\n",
    "        self.tokenizer = None\n",
    "        self.currently_loaded = set() # track currently loaded models\n",
    "\n",
    "        # get tokenizer from initial base model load\n",
    "        logger.info(\"Loading base model to obtain tokenizer...\")\n",
    "        _, self.tokenizer = self.model_loader.load_model_and_tokenizer(local_only=True)\n",
    "        if self.tokenizer.pad_token_id is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
    "\n",
    "        # unload the fp32 base model immediately\n",
    "        self.model_loader.unload_model()\n",
    "        self.cleanup_memory()\n",
    "        logger.info(\"✓ Obtained tokenizer and unloaded fp32 base model\")\n",
    "\n",
    "    def cleanup_memory(self):\n",
    "        \"\"\"Aggressive memory cleanup.\"\"\"\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            torch.cuda.synchronize()\n",
    "        logger.info(\"Memory cleanup completed\")\n",
    "\n",
    "    def load_model(self, model_name: str) -> None:\n",
    "        \"\"\"\n",
    "        Loads a model while maintaining the two-model limit.\n",
    "        Unloads other models if necessary.\n",
    "        \"\"\"\n",
    "        if model_name in self.currently_loaded:\n",
    "            return # model already loaded\n",
    "\n",
    "        # check if we need to unload any agent models\n",
    "        while len(self.currently_loaded) >= 2:\n",
    "            to_unload = next(iter(self.currently_loaded)) # get first loaded agent model\n",
    "            self.unload_model(to_unload)\n",
    "\n",
    "        logger.info(f\"Loading model: {model_name}\")\n",
    "        config = self.model_configs[model_name]\n",
    "\n",
    "        try: # load model with quantization\n",
    "            config.model = self.model_loader.reload_model(config.path)\n",
    "            config.model.eval().to(config.model.device)\n",
    "            self.currently_loaded.add(model_name) if not model_name == \"parser\" else None\n",
    "            logger.info(f\"✓ Loaded {model_name} model\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load {model_name} model: {e}\")\n",
    "            raise\n",
    "\n",
    "    def unload_model(self, model_name: str) -> None:\n",
    "        \"\"\"Unloads a specific model and clean up memory.\"\"\"\n",
    "        if model_name not in self.currently_loaded:\n",
    "            return\n",
    "\n",
    "        logger.info(f\"Unloading model: {model_name}\")\n",
    "        config = self.model_configs[model_name]\n",
    "        if hasattr(config.model, 'cpu'): # move to CPU first\n",
    "            config.model.cpu()\n",
    "        del config.model\n",
    "        config.model = None\n",
    "        self.currently_loaded.remove(model_name)\n",
    "        self.cleanup_memory()\n",
    "        logger.info(f\"✓ Unloaded {model_name} model\")\n",
    "\n",
    "    def get_model(self, model_name: str) -> torch.nn.Module:\n",
    "        \"\"\"Get a model, loading it if necessary.\"\"\"\n",
    "        if model_name not in self.currently_loaded:\n",
    "            self.load_model(model_name)\n",
    "        return self.model_configs[model_name].model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test_scenarios(\n",
    "    data_loader: object,\n",
    "    model_manager: ModelManager,\n",
    "    num_scenarios: int\n",
    ") -> List[TestResult]:\n",
    "    \"\"\"\n",
    "    Runs test scenarios with optimized model loading/unloading.\n",
    "    \"\"\"\n",
    "    scenarios = data_loader.get_batch(split='test', batch_size=num_scenarios)\n",
    "    assert len(scenarios) == num_scenarios\n",
    "    logger.info(f\"Running {num_scenarios} test scenarios...\")\n",
    "\n",
    "    # load parser model\n",
    "    parser_model = model_manager.load_model(\"parser\")\n",
    "\n",
    "    results = []\n",
    "    for scenario_dict in scenarios:\n",
    "        scenario = Scenario(scenario_dict)\n",
    "\n",
    "        # test configurations to evaluate\n",
    "        test_pairs = [\n",
    "            # Baseline Buyer vs Baseline Seller\n",
    "            (\"baseline\", \"baseline\"),\n",
    "            # Finetuned Buyer vs Baseline Seller\n",
    "            (\"buyer_finetuned\", \"baseline\")\n",
    "        ]\n",
    "\n",
    "        for buyer_model_name, seller_model_name in test_pairs:\n",
    "            logger.info(\"\\nRunning negotiation with:\")\n",
    "            logger.info(f\"Buyer: {buyer_model_name}\")\n",
    "            logger.info(f\"Seller: {seller_model_name}\")\n",
    "\n",
    "            # load required models for this negotiation\n",
    "            buyer_model = model_manager.get_model(buyer_model_name)\n",
    "            seller_model = model_manager.get_model(seller_model_name)\n",
    "\n",
    "            # create minimal configs for Agent instances\n",
    "            buyer_agent_config = ModelConfig(\n",
    "                name=buyer_model_name,\n",
    "                model_type=model_manager.model_configs[buyer_model_name].model_type,\n",
    "                path=\"\", # path not needed for loaded models\n",
    "                model=buyer_model\n",
    "            )\n",
    "            seller_agent_config = ModelConfig(\n",
    "                name=seller_model_name,\n",
    "                model_type=model_manager.model_configs[seller_model_name].model_type,\n",
    "                path=\"\", # path not needed for loaded models\n",
    "                model=seller_model\n",
    "            )\n",
    "\n",
    "            buyer = Buyer(buyer_agent_config, scenario.buyer_kb, model_manager.tokenizer)\n",
    "            seller = Seller(seller_agent_config, scenario.seller_kb, model_manager.tokenizer)\n",
    "\n",
    "            negotiation = Negotiation(\n",
    "                scenario=scenario,\n",
    "                buyer=buyer,\n",
    "                seller=seller,\n",
    "                parser_model=parser_model\n",
    "            )\n",
    "\n",
    "            result = negotiation.run()\n",
    "            results.append(TestResult(\n",
    "                buyer_model=f\"{buyer_model_name.upper()}-{buyer_agent_config.model_type}\",\n",
    "                seller_model=f\"{seller_model_name.upper()}-{seller_agent_config.model_type}\",\n",
    "                accepted=result[\"accepted\"],\n",
    "                num_turns=result[\"num_turns\"],\n",
    "                final_price=result[\"final_price\"]\n",
    "            ))\n",
    "\n",
    "            # cleanup any models not needed for next iteration\n",
    "            model_manager.cleanup_memory()\n",
    "\n",
    "    # unload parser model\n",
    "    model_manager.unload_model(\"parser\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f12G3ibtGdYJ",
    "outputId": "941a0c40-9f14-4425-d31c-f9cf7455aa28"
   },
   "outputs": [],
   "source": [
    "# define directories and model configs\n",
    "finetuning_dir = os.getcwd()\n",
    "model_configs = {\n",
    "    \"baseline\": ModelConfig(\n",
    "        name=\"baseline\",\n",
    "        model_type=\"llama-3.2-3B-instruct\",\n",
    "        path=os.path.join(finetuning_dir, \"models--meta-llama--Llama-3.2-3B-Instruct\")\n",
    "    ),\n",
    "    \"buyer_finetuned\": ModelConfig(\n",
    "        name=\"buyer_finetuned\",\n",
    "        model_type=\"llama-3.2-3B-instruct\",\n",
    "        path=os.path.join(finetuning_dir, \"models--buyer-finetuned--Llama-3.2-3B-Instruct\")\n",
    "    ),\n",
    "    \"seller_finetuned\": ModelConfig(\n",
    "        name=\"seller_finetuned\",\n",
    "        model_type=\"llama-3.2-3B-instruct\",\n",
    "        path=os.path.join(finetuning_dir, \"models--seller-finetuned--Llama-3.2-3B-Instruct\")\n",
    "    ),\n",
    "    \"generalist_finetuned\": ModelConfig(\n",
    "        name=\"generalist_finetuned\",\n",
    "        model_type=\"llama-3.2-3B-instruct\",\n",
    "        path=os.path.join(finetuning_dir, \"models--generalist-finetuned--Llama-3.2-3B-Instruct\")\n",
    "    ),\n",
    "    \"parser\": ModelConfig(\n",
    "        name=\"baseline_parser\",\n",
    "        model_type=\"llama-3.2-3B-instruct\",\n",
    "        path=os.path.join(finetuning_dir, \"models--meta-llama--Llama-3.2-3B-Instruct\")\n",
    "    )\n",
    "}\n",
    "\n",
    "# initialize loaders\n",
    "data_loader = DataLoader()\n",
    "model_loader = ModelLoader(cache_dir=model_configs[\"baseline\"].path)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "loaded_models = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5AYxOYIqGdYJ"
   },
   "source": [
    "### Run Tests and Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X-iqFYE8iB12"
   },
   "outputs": [],
   "source": [
    "def analyze_results(results: List[TestResult]) -> None:\n",
    "    \"\"\"\n",
    "    Analyzes and logs negotiation test results, providing a detailed summary and visualization.\n",
    "\n",
    "    Args:\n",
    "        results (List[TestResult]): List of structured negotiation results to analyze.\n",
    "\n",
    "    Steps:\n",
    "    1. Convert results into a DataFrame.\n",
    "    2. Group data by buyer and seller models, calculating acceptance rate, average turns, \n",
    "       and average final price.\n",
    "    3. Handle edge cases where results may be empty or contain insufficient data for aggregation.\n",
    "    4. Log the summary in tabular form for detailed inspection.\n",
    "    5. Provide detailed visualizations for better understanding of results.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If no valid results data is available for aggregation.\n",
    "    \"\"\"\n",
    "    # convert results to a DataFrame\n",
    "    df = pd.DataFrame([vars(r) for r in results])\n",
    "    if df.empty:\n",
    "        logger.warning(\"Result data is empty. Ensure valid results are passed.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # group data by buyer and seller model combinations\n",
    "        grouped_summary = df.groupby(['buyer_model', 'seller_model']).agg(\n",
    "            acceptance_rate=('accepted', 'mean'),\n",
    "            avg_turns=('num_turns', 'mean'),\n",
    "            avg_final_price=('final_price', lambda x: x[df['accepted']].mean() if len(x[df['accepted']]) > 0 else 0)\n",
    "        ).reset_index()\n",
    "\n",
    "        # log summary in tabular form\n",
    "        logger.info(\"\\nSummary of Results:\")\n",
    "        logger.info(grouped_summary.to_string(index=False))\n",
    "\n",
    "        # visualize results\n",
    "        fig, axes = plt.subplots(2, 1, figsize=(10, 12))\n",
    "\n",
    "        # bar chart for Acceptance Rate\n",
    "        grouped_summary.plot.bar(\n",
    "            x=['buyer_model', 'seller_model'],\n",
    "            y='acceptance_rate',\n",
    "            ax=axes[0],\n",
    "            legend=True,\n",
    "            title=\"Acceptance Rate by Model Pair\"\n",
    "        )\n",
    "        axes[0].set_ylabel(\"Acceptance Rate\")\n",
    "        axes[0].set_xlabel(\"Model Pairs\")\n",
    "\n",
    "        # line chart for Average Turns and Final Price\n",
    "        grouped_summary.plot(\n",
    "            x=['buyer_model', 'seller_model'],\n",
    "            y=['avg_turns', 'avg_final_price'],\n",
    "            ax=axes[1],\n",
    "            title=\"Average Turns and Final Price by Model Pair\",\n",
    "            style=['o-', 's--']\n",
    "        )\n",
    "        axes[1].set_ylabel(\"Values\")\n",
    "        axes[1].set_xlabel(\"Model Pairs\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred while analyzing results: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_manager = ModelManager(model_configs, model_loader)\n",
    "try: # run test scenarios and analyze results\n",
    "    results = run_test_scenarios(\n",
    "        data_loader=data_loader,\n",
    "        model_manager=model_manager,\n",
    "        num_scenarios=1\n",
    "    )\n",
    "    analyze_results(results)\n",
    "\n",
    "finally: # cleanup all models\n",
    "    for model_name in model_manager.currently_loaded.copy():\n",
    "        model_manager.unload_model(model_name)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

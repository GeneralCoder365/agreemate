{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8gdIuFzmGg02"
   },
   "source": [
    "### Mods for Colab Compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JPu7n7EnGiMA",
    "outputId": "c56e96e5-5aab-4636-ff3c-42d54d2f7fdd"
   },
   "outputs": [],
   "source": [
    "!pip install triton # required by TorchInductor (backend compiler in PyTorch)\n",
    "!pip install cloud-tpu-client # for tpu connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3KrPvsWPGlqU",
    "outputId": "a07c4801-d1b0-4578-876e-66718e2281e8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import drive\n",
    "\n",
    "colab = True # global indicator of colab run\n",
    "drive.flush_and_unmount() # unmount existing session\n",
    "# mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "# set working dir to finetuning dir\n",
    "os.chdir(os.path.join(\"/content/drive/My Drive\", \"agreemate\", \"finetuning\"))\n",
    "\n",
    "print(\"Working directory:\", os.getcwd())\n",
    "print(\"Files in directory:\", os.listdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gBlJLD05GdYF"
   },
   "source": [
    "# AgreeMate Model Testing Notebook\n",
    "\n",
    "Mini notebook for testing baseline and finetuned models on CraigslistBargains data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dS0E_dnmGdYG"
   },
   "outputs": [],
   "source": [
    "import os, sys, logging, re, torch\n",
    "from typing import Optional, Tuple, List, Dict, Any\n",
    "from dataclasses import dataclass\n",
    "from logging.handlers import RotatingFileHandler\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "from agreemate.baseline.utils.data_loader import DataLoader\n",
    "from model_loader import ModelLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2PFvqr98GdYH"
   },
   "source": [
    "Setup Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q5nZj3zdGdYH"
   },
   "outputs": [],
   "source": [
    "class TqdmCompatibleStreamHandler(logging.StreamHandler):\n",
    "    \"\"\"\n",
    "    Custom logging handler that avoids interfering with tqdm progress bars.\n",
    "    \"\"\"\n",
    "    def emit(self, record):\n",
    "        try:\n",
    "            msg = self.format(record)\n",
    "            tqdm.write(msg) # use tqdm's write method for compatibility\n",
    "            self.flush()\n",
    "        except Exception:\n",
    "            self.handleError(record)\n",
    "\n",
    "# file handler for logging everything to a file (limit size to 5MB)\n",
    "file_handler = RotatingFileHandler(\n",
    "    os.path.join(os.getcwd(), \"tests.log\"),\n",
    "    maxBytes=10**6, backupCount=5\n",
    ")\n",
    "file_handler.setLevel(logging.DEBUG)\n",
    "\n",
    "# console handler for logging to notebook's output\n",
    "console_handler = TqdmCompatibleStreamHandler()\n",
    "console_handler.setLevel(logging.INFO)\n",
    "\n",
    "# set up the logging configuration\n",
    "logging.basicConfig(\n",
    "    handlers=[file_handler, console_handler],\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    level=logging.DEBUG,\n",
    "    force=True, # force logging reconfig\n",
    ")\n",
    "\n",
    "# create logger\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# suppress overly verbose logs in notebook output\n",
    "logging.getLogger(\"transformers\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"datasets\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"torch\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"accelerate\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v7BL4jaEGdYH"
   },
   "source": [
    "## Scenario Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JCFZKcvUGdYH"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Scenario:\n",
    "    \"\"\"Formats raw scenario data into structured buyer/seller knowledge bases.\"\"\"\n",
    "    def __init__(self, scenario_dict):\n",
    "        self.title = scenario_dict['item']['title']\n",
    "        self.description = scenario_dict['item']['description']\n",
    "        self.category = scenario_dict['category']\n",
    "        self.listing_price = scenario_dict['item']['list_price']\n",
    "        self.buyer_target = scenario_dict['buyer']['target_price']\n",
    "        self.seller_target = scenario_dict['seller']['target_price']\n",
    "\n",
    "        self.buyer_kb = {\n",
    "            \"role\": \"buyer\",\n",
    "            \"title\": self.title,\n",
    "            \"description\": self.description,\n",
    "            \"category\": self.category,\n",
    "            \"listing_price\": self.listing_price,\n",
    "            \"target_price\": self.buyer_target,\n",
    "            \"relative_price\": scenario_dict['buyer']['relative_price']\n",
    "        }\n",
    "\n",
    "        self.seller_kb = {\n",
    "            \"role\": \"seller\",\n",
    "            \"title\": self.title,\n",
    "            \"description\": self.description,\n",
    "            \"category\": self.category,\n",
    "            \"listing_price\": self.listing_price,\n",
    "            \"target_price\": self.seller_target,\n",
    "            \"price_delta_pct\": scenario_dict['seller']['price_delta_pct']\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xcK6QF7JGdYI"
   },
   "outputs": [],
   "source": [
    "class Dialogue:\n",
    "    \"\"\"Single dialogue turn in negotiation.\"\"\"\n",
    "    def __init__(self, utterance, action, price=None, agent=\"unknown\"):\n",
    "        self.utterance = utterance\n",
    "        self.action = action\n",
    "        self.price = price if action in {\"price\", \"counter\", \"emphasize\"} else None\n",
    "        self.agent = agent\n",
    "        self.current_price = None\n",
    "\n",
    "    def extract_price(self):\n",
    "        \"\"\"Extracts price from utterance if present.\"\"\"\n",
    "        matched_price = re.search(r'\\$\\s?(\\d+(\\.\\d+)?)', self.utterance)\n",
    "        if matched_price:\n",
    "            self.price = float(matched_price.group(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ycBkVMG9GdYI"
   },
   "outputs": [],
   "source": [
    "class Negotiation:\n",
    "    \"\"\"Manages complete negotiation dialogue between agents.\"\"\"\n",
    "    def __init__(self, scenario, buyer, seller, base_model):\n",
    "        self.scenario = scenario\n",
    "        self.buyer = buyer\n",
    "        self.seller = seller\n",
    "        self.base_model = base_model\n",
    "        self.conversation_history = []\n",
    "        self.prepared_conversation_history = \"\"\n",
    "\n",
    "        self.accepted = False\n",
    "        self.accepted_price = None\n",
    "\n",
    "    def add_to_history(self, dialogue):\n",
    "        \"\"\"Adds turn to conversation history.\"\"\"\n",
    "        self.conversation_history.append(dialogue)\n",
    "        self.prepared_conversation_history = \"\\n\".join(\n",
    "            f\"{d.agent.capitalize()}: {d.utterance}\" for d in self.conversation_history\n",
    "        )\n",
    "\n",
    "    def run(self, max_turns=10):\n",
    "        \"\"\"Executes negotiation dialogue and return results.\"\"\"\n",
    "        logger.info(f\"\\nNegotiation for: {self.scenario.title}\")\n",
    "        logger.info(f\"List Price: ${self.scenario.listing_price}\")\n",
    "        logger.info(\"=\"*80)\n",
    "\n",
    "        turn = 0\n",
    "        current_price = None\n",
    "\n",
    "        while turn < max_turns:\n",
    "            agent = self.buyer if turn % 2 == 0 else self.seller\n",
    "            dialogue = agent(self.prepared_conversation_history,\n",
    "                                      (max_turns - turn) // 2, self.base_model)\n",
    "\n",
    "            logger.info(f\"{dialogue.agent.capitalize()}: {dialogue.utterance}\")\n",
    "            self.add_to_history(dialogue)\n",
    "            turn += 1\n",
    "\n",
    "            if dialogue.price:\n",
    "                current_price = dialogue.price\n",
    "            if dialogue.action == \"accept\":\n",
    "                self.accepted = True\n",
    "                self.accepted_price = current_price\n",
    "                break\n",
    "            if dialogue.action == \"reject\":\n",
    "                self.accepted = False\n",
    "                break\n",
    "\n",
    "        logger.info(\"~\"*80)\n",
    "        logger.info(f\"Outcome: {'Accepted' if self.accepted else 'Rejected'}\")\n",
    "        if self.accepted:\n",
    "            logger.info(f\"Final Price: ${self.accepted_price}\")\n",
    "        logger.info(\"=\"*80 + \"\\n\")\n",
    "\n",
    "        return {\n",
    "            \"accepted\": self.accepted,\n",
    "            \"final_price\": self.accepted_price,\n",
    "            \"num_turns\": len(self.conversation_history),\n",
    "            \"listing_price\": self.scenario.listing_price,\n",
    "            \"buyer_target\": self.scenario.buyer_target,\n",
    "            \"seller_target\": self.scenario.seller_target\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWfvSB73GdYI"
   },
   "source": [
    "## Prompting and Response Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SlZ6EgL6GdYI"
   },
   "source": [
    "Define system prompt and role-specific instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eAh9BB0_GdYI"
   },
   "outputs": [],
   "source": [
    "def get_buyer_instructions(kb: dict) -> str:\n",
    "    \"\"\"Returns buyer-specific instructions.\"\"\"\n",
    "    return f\"\"\"\n",
    "    ### Your Role as the Buyer:\n",
    "    - Goal: Negotiate as close to your target price (${kb['target_price']}) as possible.\n",
    "    - Strategy:\n",
    "        1. Begin with offers below your target price but reasonable enough to engage the seller.\n",
    "        2. Highlight flaws or alternatives to justify lower offers.\n",
    "        3. Avoid exceeding your target price, even if pressured.\n",
    "    - Be respectful but firm. Do not overcommit.\n",
    "\n",
    "    ### Example Buyer Responses:\n",
    "    1. Thought: I believe the item is worth $150 based on its flaws.\n",
    "       Action: offer\n",
    "       Utterance: I can offer $150 considering the wear and tear.\n",
    "    2. Thought: The seller's offer aligns with my target price.\n",
    "       Action: accept\n",
    "       Utterance: $200 works for me. Let's finalize this deal.\n",
    "    \"\"\"\n",
    "\n",
    "def get_seller_instructions(kb: dict) -> str:\n",
    "    \"\"\"Returns seller-specific instructions.\"\"\"\n",
    "    return f\"\"\"\n",
    "    ### Your Role as the Seller:\n",
    "    - Goal: Maximize the final price while keeping the buyer engaged.\n",
    "    - Strategy:\n",
    "        1. Anchor the negotiation around the listing price (${kb['listing_price']}).\n",
    "        2. Justify your price by emphasizing the item's quality or rarity.\n",
    "        3. Counter offers strategically, staying above your target price (${kb['target_price']}).\n",
    "\n",
    "    ### Example Seller Responses:\n",
    "    1. Thought: The buyer's offer is too low, but I want to keep negotiating.\n",
    "       Action: counter\n",
    "       Utterance: I can't do $150, but I can offer $180 given the item's excellent condition.\n",
    "    2. Thought: This is my lowest acceptable price; I'm ready to conclude.\n",
    "       Action: emphasize\n",
    "       Utterance: This item is priced competitively at $200 due to its rarity.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "def get_base_prompt(kb: dict, conversation_history: str, turns_left: int) -> str:\n",
    "    \"\"\"Generates a base prompt with common agent context and instructions.\"\"\"\n",
    "    return f\"\"\"\n",
    "    ### Negotiation Scenario:\n",
    "    - Item: {kb['title']}\n",
    "    - Description: {kb['description']}\n",
    "    - List Price: ${kb['listing_price']}\n",
    "    - Your Role: {kb['role'].capitalize()}\n",
    "    - Turns Left to Conclude: {turns_left}\n",
    "\n",
    "    ### Previous Conversation:\n",
    "    {conversation_history}\n",
    "\n",
    "    ### Response Instructions:\n",
    "    1. Consider the negotiation context and role-specific goals.\n",
    "    2. Think aloud about your reasoning before deciding your action.\n",
    "    3. Respond strictly in the following structured format:\n",
    "    ---\n",
    "    Thought: <your reasoning>\n",
    "    Action: <offer|counter|accept|reject>\n",
    "    Utterance: <your message>\n",
    "    (Optional) Price: $<price>\n",
    "    ---\n",
    "    Respond below:\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "def get_parser_prompt(response: str) -> str:\n",
    "    \"\"\"Generates a parsing prompt for the base model to structure an agent's response.\"\"\"\n",
    "    return f\"\"\"\n",
    "    ### Agent's Response (Free-form):\n",
    "    ---\n",
    "    {response}\n",
    "    ---\n",
    "\n",
    "    ### Parsing Instructions:\n",
    "    - Extract and structure the response using the following format:\n",
    "    ---\n",
    "    Action: <offer|counter|accept|reject>\n",
    "    Utterance: <the agent's message>\n",
    "    (Optional) Price: $<price>\n",
    "    ---\n",
    "    Respond in this exact format without any additional information.\n",
    "\n",
    "    Structured Output:\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gnuGIx-PGdYI"
   },
   "source": [
    "## Agent Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fh8t_1_FcTJ1"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Configuration for a model including path and type information.\"\"\"\n",
    "    name: str\n",
    "    model_type: str\n",
    "    path: str\n",
    "    model: Optional[object] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7xTg3t4UGdYI"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"Base agent class for negotiation participants.\"\"\"\n",
    "    def __init__(self, config: ModelConfig, role: str, kb: dict, tokenizer: object):\n",
    "        self.role = role\n",
    "        self.config = config\n",
    "        self.kb = kb\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    #! PROMPTING\n",
    "    def truncate_conversation(self, conversation_history: str, max_response_tokens: int = 100) -> str:\n",
    "        \"\"\"Truncates conversation history dynamically to fit within model token limits.\"\"\"\n",
    "        # get model's max token limit\n",
    "        max_tokens = getattr(self.config.model.config, \"max_position_embeddings\", 8192)\n",
    "\n",
    "        # tokenize base prompt and role instructions to calculate their size\n",
    "        base_prompt_tokens = len(self.tokenizer.encode(get_base_prompt(self.kb, \"\", 0), add_special_tokens=False))\n",
    "        role_instructions_tokens = len(self.tokenizer.encode(\n",
    "            get_buyer_instructions(self.kb) if self.role == \"buyer\" else get_seller_instructions(self.kb),\n",
    "            add_special_tokens=False\n",
    "        ))\n",
    "\n",
    "        # calculate available space for conversation history\n",
    "        reserved_tokens = 5 # BOS + EOS + any other special tokens (safe margin)\n",
    "        available_tokens = max_tokens - reserved_tokens - base_prompt_tokens - role_instructions_tokens - max_response_tokens\n",
    "        if available_tokens <= 0:\n",
    "            raise ValueError(\"Prompt components exceed the model's token limit!\")\n",
    "\n",
    "        conversation_tokens = self.tokenizer.encode(conversation_history, add_special_tokens=False) # tokenize history\n",
    "\n",
    "        if len(conversation_tokens) > available_tokens: # truncate conversation history if necessary and return\n",
    "            logger.info(\"Truncating conversation history to fit within token limits\")\n",
    "            logger.info(f\"Available tokens: {available_tokens} < {len(conversation_tokens)} (history length)\")\n",
    "            return self.tokenizer.decode(conversation_tokens[-available_tokens:], skip_special_tokens=True)\n",
    "\n",
    "        return conversation_history # return original history if it fits\n",
    "\n",
    "    def get_prompt(self, conversation_history: str, turns_left: int) -> str:\n",
    "        \"\"\"Stitches together role-specific prompt.\"\"\"\n",
    "        truncated_history = self.truncate_conversation(conversation_history) # truncate history if needed\n",
    "\n",
    "        # get base prompt and role-specific instructions\n",
    "        base_prompt = get_base_prompt(self.kb, truncated_history, turns_left)\n",
    "        role_instructions = (get_buyer_instructions if self.role == \"buyer\"\n",
    "                                                    else get_seller_instructions)(self.kb)\n",
    "\n",
    "        # combine role-specific instructions with base prompt\n",
    "        return f\"{role_instructions}\\n{base_prompt}\"\n",
    "\n",
    "\n",
    "    #! RESPONSE GENERATION AND PARSING\n",
    "    def generate_response(self, prompt: str, max_tokens: int = 150) -> str:\n",
    "        \"\"\"Generates response to prompt using the model.\"\"\"\n",
    "        inputs = self.tokenizer(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        ).to(self.config.model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.config.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_tokens,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                pad_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        response = self.tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:],\n",
    "                                                          skip_special_tokens=True)\n",
    "        return response\n",
    "\n",
    "    def parse_response(self, raw_response: str, base_model) -> tuple:\n",
    "        \"\"\"Parse raw agent response into structured components (action, utterance, price) using the base model.\"\"\"\n",
    "        logger.info(f\"Parsing Raw Response:\\n{raw_response}\\n\")\n",
    "\n",
    "        # prepare for parsing\n",
    "        parser_prompt = get_parser_prompt(raw_response)\n",
    "\n",
    "        # generate a structured response using the base model\n",
    "        inputs = self.tokenizer(\n",
    "            parser_prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        ).to(base_model.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = base_model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=20,\n",
    "                pad_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "        parsed_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        logger.info(f\"Parsed Response:\\n{parsed_response}\\n\")\n",
    "\n",
    "        # extract components directly from the structured response\n",
    "        components = {\"action\": None, \"utterance\": None, \"price\": None}\n",
    "        for line in parsed_response.split(\"\\n\"):\n",
    "            line = line.strip().lower()\n",
    "            if not line:\n",
    "                continue\n",
    "            for key in components:\n",
    "                if line.startswith(f\"{key}:\"):\n",
    "                    value = line.split(\":\", 1)[1].strip()\n",
    "                    if key == \"price\":\n",
    "                        try:\n",
    "                            value = float(value.replace(\"$\", \"\"))\n",
    "                        except ValueError:\n",
    "                            logger.warning(f\"Invalid price format: {value}\")\n",
    "                            value = None\n",
    "                    components[key] = value\n",
    "                    break\n",
    "\n",
    "        if not components[\"action\"] or not components[\"utterance\"]:\n",
    "            raise ValueError(f\"Invalid response format: {parsed_response}\")\n",
    "        return (components[\"utterance\"], components[\"action\"], components[\"price\"])\n",
    "\n",
    "\n",
    "    #! TURN\n",
    "    def __call__(self, conversation_history: str, turns_left: int, base_model) -> Dialogue:\n",
    "        \"\"\"Generate and parse agent response.\"\"\"\n",
    "        # generate agent response\n",
    "        prompt = self.get_prompt(conversation_history, turns_left)\n",
    "        raw_response = self.generate_response(prompt)\n",
    "\n",
    "        # parse into structured components\n",
    "        utterance, action, price = self.parse_response(raw_response, base_model)\n",
    "        logger.info(f\"Parsed Response - Action: {action}, Utterance: {utterance}, Price: {price if price else 'None'}\")\n",
    "\n",
    "        return Dialogue(utterance, action, price, self.role)\n",
    "\n",
    "\n",
    "class Buyer(Agent):\n",
    "    \"\"\"Buyer Agent.\"\"\"\n",
    "    def __init__(self, config: ModelConfig, kb: dict, tokenizer: object):\n",
    "        super().__init__(config, \"buyer\", kb, tokenizer)\n",
    "\n",
    "class Seller(Agent):\n",
    "    \"\"\"Seller Agent.\"\"\"\n",
    "    def __init__(self, config: ModelConfig, kb: dict, tokenizer: object):\n",
    "        super().__init__(config, \"seller\", kb, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D9FUg7MJGdYJ"
   },
   "source": [
    "## Run Test Negotiations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EpV_Py_Ee-wP"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TestResult:\n",
    "    \"\"\"Structured result from a negotiation test scenario.\"\"\"\n",
    "    buyer_model: str\n",
    "    seller_model: str\n",
    "    accepted: bool\n",
    "    num_turns: int\n",
    "    final_price: Optional[float]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BUYd2wCKGdYJ"
   },
   "outputs": [],
   "source": [
    "def run_test_scenarios(data_loader: object, model_configs: Dict[str, ModelConfig], tokenizer: object, num_scenarios: int):\n",
    "    \"\"\"Runs test negotiations using both baseline and finetuned models.\"\"\"\n",
    "    scenarios = data_loader.get_batch(split='test', batch_size=num_scenarios)\n",
    "    assert len(scenarios) == num_scenarios\n",
    "    logger.info(f\"Running {num_scenarios} test scenarios...\")\n",
    "\n",
    "    results = []\n",
    "    for scenario_dict in scenarios:\n",
    "        scenario = Scenario(scenario_dict)\n",
    "\n",
    "        # test configurations to evaluate\n",
    "        test_pairs = [\n",
    "            # Baseline Buyer vs Baseline Seller\n",
    "            (model_configs[\"baseline\"], model_configs[\"baseline\"]),\n",
    "            # Finetuned Buyer vs Baseline Seller\n",
    "            (model_configs[\"buyer_finetuned\"], model_configs[\"baseline\"])\n",
    "        ]\n",
    "\n",
    "        for buyer_config, seller_config in test_pairs: # run negotiation for each pair\n",
    "            if not buyer_config.model:\n",
    "                raise ValueError(f\"Model not loaded for buyer: {buyer_config.name.upper()} ({buyer_config.model_type})\")\n",
    "            if not seller_config.model:\n",
    "                raise ValueError(f\"Model not loaded for seller: {seller_config.name.upper()} ({seller_config.model_type})\")\n",
    "\n",
    "            # initialize agents and negotiation instance\n",
    "            buyer = Buyer(buyer_config, scenario.buyer_kb, tokenizer)\n",
    "            seller = Seller(seller_config, scenario.seller_kb, tokenizer)\n",
    "            negotiation = Negotiation(\n",
    "                scenario=scenario,\n",
    "                buyer=buyer,\n",
    "                seller=seller,\n",
    "                base_model=model_configs[\"baseline\"].model\n",
    "            )\n",
    "            logger.info(f\"Running negotiation for: {scenario.title}\")\n",
    "            logger.info(f\"with Buyer: {buyer_config.name.upper()}_{buyer_config.model_type}\")\n",
    "            logger.info(f\"and Seller: {seller_config.name.upper()}_{seller_config.model_type}\")\n",
    "\n",
    "            # run negotiation and store results\n",
    "            result = negotiation.run()\n",
    "            results.append(TestResult(\n",
    "                buyer_model=f\"{buyer_config.name.upper()}_{buyer_config.model_type}\",\n",
    "                seller_model=f\"{seller_config.name.upper()}_{seller_config.model_type}\",\n",
    "                accepted=result[\"accepted\"],\n",
    "                num_turns=result[\"num_turns\"],\n",
    "                final_price=result[\"final_price\"]\n",
    "            ))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WPSZ0ttwGdYJ"
   },
   "source": [
    "### Initialize Loaders and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f12G3ibtGdYJ",
    "outputId": "941a0c40-9f14-4425-d31c-f9cf7455aa28"
   },
   "outputs": [],
   "source": [
    "# define directories and model configs\n",
    "finetuning_dir = os.getcwd()\n",
    "model_configs = {\n",
    "    \"baseline\": ModelConfig(\n",
    "        name=\"baseline\",\n",
    "        model_type=\"llama-3.1-8B-instruct\",\n",
    "        path=os.path.join(finetuning_dir, \"models--meta-llama--Llama-3.1-8B-Instruct\")\n",
    "    ),\n",
    "    \"buyer_finetuned\": ModelConfig(\n",
    "        name=\"buyer_finetuned\",\n",
    "        model_type=\"llama-3.1-8B-instruct\",\n",
    "        path=os.path.join(finetuning_dir, \"models--buyer-finetuned--Llama-3.1-8B-Instruct\")\n",
    "    ),\n",
    "    \"seller_finetuned\": ModelConfig(\n",
    "        name=\"seller_finetuned\",\n",
    "        model_type=\"llama-3.1-8B-instruct\",\n",
    "        path=os.path.join(finetuning_dir, \"models--seller-finetuned--Llama-3.1-8B-Instruct\")\n",
    "    ),\n",
    "    \"generalist_finetuned\": ModelConfig(\n",
    "        name=\"generalist_finetuned\",\n",
    "        model_type=\"llama-3.1-8B-instruct\",\n",
    "        path=os.path.join(finetuning_dir, \"models--generalist-finetuned--Llama-3.1-8B-Instruct\")\n",
    "    )\n",
    "}\n",
    "\n",
    "# initialize loaders\n",
    "data_loader = DataLoader()\n",
    "model_loader = ModelLoader(cache_dir=model_configs[\"baseline\"].path)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "loaded_models = []\n",
    "\n",
    "# load models\n",
    "logger.info(\"Loading models...\")\n",
    "for config_name, config in model_configs.items():\n",
    "    try:\n",
    "        if config_name == \"baseline\":\n",
    "            config.model, base_tokenizer = model_loader.load_model_and_tokenizer(local_only=True)\n",
    "            # set padding token to eos token if not already set\n",
    "            if base_tokenizer.pad_token_id is None:\n",
    "                base_tokenizer.pad_token = base_tokenizer.eos_token\n",
    "                base_tokenizer.pad_token_id = base_tokenizer.eos_token_id\n",
    "        else:\n",
    "            config.model = model_loader.reload_model(config.path)\n",
    "        config.model.eval().to(device)\n",
    "        loaded_models.append(config.name)\n",
    "        logger.info(f\"Loaded {config.name.upper()} model to {config.model.device}\")\n",
    "    except Exception as e: # don't halt execution if a model fails to load\n",
    "        logger.error(f\"Failed to load {config.name.upper()} model: {e}\")\n",
    "\n",
    "if \"baseline\" not in loaded_models:\n",
    "    raise ValueError(\"✗ Failed to load baseline model, which is required for testing.\")\n",
    "logger.info(f\"✓ Models loaded successfully: {', '.join(model.upper() for model in loaded_models)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5AYxOYIqGdYJ"
   },
   "source": [
    "### Run Tests and Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 998
    },
    "id": "L3eRQSigGdYJ",
    "outputId": "a6639a7f-3850-4eae-a160-bce5ad52dfc0"
   },
   "outputs": [],
   "source": [
    "results = run_test_scenarios(data_loader=data_loader, model_configs=model_configs, tokenizer=base_tokenizer, num_scenarios=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X-iqFYE8iB12"
   },
   "outputs": [],
   "source": [
    "def analyze_results(results: List[TestResult]) -> None:\n",
    "    \"\"\"Analyzes and logs negotiation test results.\"\"\"\n",
    "    df = pd.DataFrame([vars(r) for r in results])\n",
    "    summary = df.groupby(['buyer_model', 'seller_model']).agg({\n",
    "        'accepted': 'mean',\n",
    "        'num_turns': 'mean',\n",
    "        'final_price': lambda x: x[df['accepted']].mean()\n",
    "    }).rename(columns={\n",
    "        'accepted': 'Acceptance Rate',\n",
    "        'num_turns': 'Average Turns',\n",
    "        'final_price': 'Average Final Price'\n",
    "    })\n",
    "\n",
    "    logger.info(\"\\nResults Summary:\")\n",
    "    logger.info(summary)\n",
    "\n",
    "analyze_results(results)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AgreeMate Finetuner Notebook\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [Setup and Initialization](#setup-and-initialization)\n",
    "3. [Data Loading and Preparation](#data-loading-and-preparation)\n",
    "4. [Model Loading](#model-loading)\n",
    "5. [Dataset Creation](#dataset-creation)\n",
    "6. [Training Configuration](#training-configuration)\n",
    "7. [Model Fine-tuning](#model-fine-tuning)\n",
    "8. [Finetuned Models Evaluation](#evaluate-finetuned-models)\n",
    "\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "Welcome to the **AgreeMate Finetuner Notebook**. This notebook is designed to fine-tune the **Llama-3.2-1B-Instruct** model for specialized negotiation roles within bargaining environments. The objective is to create three distinct model variants:\n",
    "\n",
    "1. **Buyer Specialist**: Optimized for buyer-specific negotiation strategies.\n",
    "2. **Seller Specialist**: Optimized for seller-specific negotiation strategies.\n",
    "3. **Generalist Negotiator**: Capable of handling both buyer and seller roles effectively.\n",
    "\n",
    "This finetuning process leverages existing modules (`model_loader.py` and `data_loader.py`) to ensure seamless integration and efficient path management within the Jupyter environment. Additionally, the notebook incorporates robust checkpointing mechanisms to prevent loss of progress in case of unexpected failures.\n",
    "\n",
    "## 2. Setup and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'data_loader'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b0c9fb4f10da>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdamW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdata_loader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNegotiationDialogueDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel_loader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModelLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'data_loader'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os, logging, shutil, torch\n",
    "from copy import deepcopy\n",
    "from logging.handlers import RotatingFileHandler\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import TrainingArguments, get_linear_schedule_with_warmup\n",
    "from accelerate import Accelerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128,expandable_segments:True\"\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from data_loader import NegotiationDialogueDataLoader\n",
    "from model_loader import ModelLoader\n",
    "\n",
    "\n",
    "# configure logging\n",
    "handler = RotatingFileHandler('./progress/finetuner.log', maxBytes=10**6, backupCount=5)\n",
    "logging.basicConfig(\n",
    "    handlers=[handler], # limit log file size to 5MB\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    level=logging.DEBUG\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.info(\"Starting AgreeMate Finetuner Notebook\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logger.info(f\"Training on {device}\")\n",
    "\n",
    "\n",
    "# define directories\n",
    "finetuning_dir = os.getcwd()\n",
    "pretrained_dir = os.path.join(finetuning_dir, 'models--meta-llama--Llama-3.2-1B-Instruct')\n",
    "buyer_finetuned_dir = os.path.join(finetuning_dir, 'models--buyer-finetuned--Llama-3.2-1B-Instruct')\n",
    "seller_finetuned_dir = os.path.join(finetuning_dir, 'models--seller-finetuned--Llama-3.2-1B-Instruct')\n",
    "generalist_finetuned_dir = os.path.join(finetuning_dir, 'models--generalist-finetuned--Llama-3.2-1B-Instruct')\n",
    "os.makedirs(buyer_finetuned_dir, exist_ok=True)\n",
    "os.makedirs(seller_finetuned_dir, exist_ok=True)\n",
    "os.makedirs(generalist_finetuned_dir, exist_ok=True)\n",
    "logger.info(f\"Finetuned models will be saved to: {finetuning_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Preparation\n",
    "\n",
    "Use the existing `NegotiationDialogueDataLoader` to load and prepare the datasets for finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = NegotiationDialogueDataLoader()\n",
    "logger.info(\"Initialized NegotiationDialogueDataLoader\")\n",
    "\n",
    "# load datasets\n",
    "buyer_df = data_loader.load_dataset(\"buyer\")\n",
    "seller_df = data_loader.load_dataset(\"seller\")\n",
    "generalist_df = data_loader.load_dataset(\"generalist\")\n",
    "logger.info(f\"Loaded buyer dataset with {len(buyer_df)} examples\")\n",
    "logger.info(f\"Loaded seller dataset with {len(seller_df)} examples\")\n",
    "logger.info(f\"Loaded generalist dataset with {len(generalist_df)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Finetuning Data\n",
    "\n",
    "Format the loaded data into input-target pairs suitable for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buyer_data = data_loader.prepare_for_training(buyer_df)\n",
    "seller_data = data_loader.prepare_for_training(seller_df)\n",
    "generalist_data = data_loader.prepare_for_training(generalist_df)\n",
    "logger.info(\"Prepared training data for buyer, seller, and generalist models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Loading\n",
    "\n",
    "Use the provided `ModelLoader` to handle model loading and caching.\n",
    "\n",
    "Load the base Llama-3.2-1B-Instruct model, which will be finetuned for each specific role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loader = ModelLoader()\n",
    "logger.info(\"Initialized ModelLoader\")\n",
    "\n",
    "# load base model and tokenizer\n",
    "base_model, base_tokenizer = model_loader.load_model_and_tokenizer()\n",
    "if base_tokenizer.pad_token_id is None: # set pad token to eos token\n",
    "    base_tokenizer.pad_token = base_tokenizer.eos_token\n",
    "    base_tokenizer.pad_token_id = base_tokenizer.eos_token_id\n",
    "torch.cuda.empty_cache() # clear any residual memory\n",
    "torch.cuda.reset_peak_memory_stats() # reset memory tracking stats\n",
    "logger.info(\"Loaded base Llama-3.2-1B-Instruct model and tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dataset Creation\n",
    "\n",
    "### Define Custom Dataset Class\n",
    "\n",
    "Define a `NegotiationDataset` class to interface with Hugging Face's `Trainer`. This class handles the encoding of inputs and targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegotiationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for Negotiation Finetuning.\n",
    "\n",
    "    Attributes:\n",
    "        encodings (Dict): Tokenized inputs.\n",
    "        labels (List[List[int]]): Tokenized target responses.\n",
    "    \"\"\"\n",
    "    def __init__(self, encodings, labels):\n",
    "        \"\"\"\n",
    "        Initializes the dataset with encodings and labels.\n",
    "\n",
    "        Args:\n",
    "            encodings (Dict): Tokenized input data.\n",
    "            labels (List[List[int]]): Tokenized target responses.\n",
    "        \"\"\"\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves the input-target pair at the specified index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the data point.\n",
    "\n",
    "        Returns:\n",
    "            Dict: A dictionary containing input_ids, attention_mask, and labels.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'input_ids': self.encodings['input_ids'][idx].clone().detach(),\n",
    "            'attention_mask': self.encodings['attention_mask'][idx].clone().detach(),\n",
    "            'labels': self.labels[idx].clone().detach()\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of examples in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: Number of examples.\n",
    "        \"\"\"\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data and Tokenize Inputs and Targets\n",
    "\n",
    "Split the prepared data into training and validation sets at a 90-10 ratio.\n",
    "Tokenize the prepared data using the respective tokenizers for each model variant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(data, test_size=0.1):\n",
    "    \"\"\"Splits the dataset into training and validation sets.\"\"\"\n",
    "    inputs_train, inputs_val, labels_train, labels_val = train_test_split(\n",
    "        data['input'], data['target'], test_size=test_size, random_state=42\n",
    "    )\n",
    "    return inputs_train, inputs_val, labels_train, labels_val\n",
    "\n",
    "def tokenize_data(inputs, targets, tokenizer):\n",
    "    \"\"\"Tokenizes input and target data by concatenating them and setting labels to -100 for input tokens.\"\"\"\n",
    "\n",
    "    # concatenate inputs and targets with a separator (e.g., space or special token if needed)\n",
    "    concatenated = [f\"{inp} {tgt}\" for inp, tgt in zip(inputs, targets)]\n",
    "\n",
    "    # tokenize the concatenated sequences\n",
    "    encodings = tokenizer(\n",
    "        concatenated,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    # tokenize inputs separately to determine the boundary between input and target tokens\n",
    "    input_encodings = tokenizer(\n",
    "        inputs,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    # initialize labels with -100 (representing tokens to be ignored in loss computation)\n",
    "    labels = torch.full_like(encodings['input_ids'], -100)\n",
    "\n",
    "    # set labels for target tokens\n",
    "    for i in range(len(inputs)):\n",
    "        input_len = input_encodings['input_ids'][i].ne(tokenizer.pad_token_id).sum().item()\n",
    "        # ensure we don't exceed the sequence length\n",
    "        target_len = encodings['input_ids'][i].ne(tokenizer.pad_token_id).sum().item() - input_len\n",
    "        if target_len > 0:\n",
    "            labels[i, input_len:input_len + target_len] = encodings['input_ids'][i, input_len:input_len + target_len]\n",
    "\n",
    "    return NegotiationDataset(encodings, labels)\n",
    "\n",
    "def create_dataloaders(train_dataset, val_dataset, batch_size):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "# buyer, seller, and generalist dataset splitting\n",
    "buyer_inputs_train, buyer_inputs_val, buyer_labels_train, buyer_labels_val = split_dataset(buyer_data)\n",
    "seller_inputs_train, seller_inputs_val, seller_labels_train, seller_labels_val = split_dataset(seller_data)\n",
    "generalist_inputs_train, generalist_inputs_val, generalist_labels_train, generalist_labels_val = split_dataset(generalist_data)\n",
    "\n",
    "# tokenize datasets\n",
    "buyer_train_dataset = tokenize_data(buyer_inputs_train, buyer_labels_train, base_tokenizer)\n",
    "buyer_val_dataset = tokenize_data(buyer_inputs_val, buyer_labels_val, base_tokenizer)\n",
    "seller_train_dataset = tokenize_data(seller_inputs_train, seller_labels_train, base_tokenizer)\n",
    "seller_val_dataset = tokenize_data(seller_inputs_val, seller_labels_val, base_tokenizer)\n",
    "generalist_train_dataset = tokenize_data(generalist_inputs_train, generalist_labels_train, base_tokenizer)\n",
    "generalist_val_dataset = tokenize_data(generalist_inputs_val, generalist_labels_val, base_tokenizer)\n",
    "\n",
    "# create dataloaders\n",
    "batch_size = 1\n",
    "buyer_train_loader, buyer_val_loader = create_dataloaders(buyer_train_dataset, buyer_val_dataset, batch_size)\n",
    "seller_train_loader, seller_val_loader = create_dataloaders(seller_train_dataset, seller_val_dataset, batch_size)\n",
    "generalist_train_loader, generalist_val_loader = create_dataloaders(generalist_train_dataset, generalist_val_dataset, batch_size)\n",
    "\n",
    "logger.info(\"Created DataLoaders for buyer, seller, and generalist models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Configuration\n",
    "\n",
    "Set up the training parameters, including learning rate, batch size, number of epochs, and checkpointing strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base training arguments\n",
    "base_training_args = {\n",
    "    \"num_train_epochs\": 2,               # number of training epochs\n",
    "    \"per_device_train_batch_size\": 1,    # batch size per device during training\n",
    "    \"per_device_eval_batch_size\": 1,     # batch size for evaluation\n",
    "    \"gradient_accumulation_steps\": 16,    # accumulates gradients over 8 steps, simulating batch size of 1*16=16\n",
    "    \"fp16\": True,                        # use mixed precision training to reduce memory usage\n",
    "    \"warmup_steps\": 500,                 # number of warmup steps for learning rate scheduler\n",
    "    \"learning_rate\": 5e-5,               # learning rate for optimizer\n",
    "    \"weight_decay\": 0.01,                # weight decay for optimizer\n",
    "    \"logging_steps\": 10,                 # log every 10 steps\n",
    "    \"eval_strategy\": \"steps\",            # evaluate every few steps\n",
    "    \"save_strategy\": \"steps\",            # save model every few steps (CHECKPOINTS)\n",
    "    \"save_steps\": 100,                   # save checkpoint every 100 steps\n",
    "    \"load_best_model_at_end\": True,      # load/save the best model when finished training (also works with CHECKPOINTS)\n",
    "    \"save_total_limit\": 3,               # only keep the last 3 checkpoints to save disk space\n",
    "    \"report_to\": \"none\"                  # disable reporting to WandB or other services\n",
    "}\n",
    "\n",
    "# buyer, seller, generalist output, logging specifications\n",
    "buyer_training_args = TrainingArguments(\n",
    "    output_dir=os.path.join(finetuning_dir, 'progress', 'buyer'),\n",
    "    logging_dir=os.path.join(finetuning_dir, 'progress', 'buyer', 'logs'),\n",
    "    **base_training_args\n",
    ")\n",
    "seller_training_args = TrainingArguments(\n",
    "    output_dir=os.path.join(finetuning_dir, 'progress', 'seller'),\n",
    "    logging_dir=os.path.join(finetuning_dir, 'progress', 'seller', 'logs'),\n",
    "    **base_training_args\n",
    ")\n",
    "generalist_training_args = TrainingArguments(\n",
    "    output_dir=os.path.join(finetuning_dir, 'progress', 'generalist'),\n",
    "    logging_dir=os.path.join(finetuning_dir, 'progress', 'generalist', 'logs'),\n",
    "    **base_training_args\n",
    ")\n",
    "\n",
    "logger.info(\"Defined TrainingArguments for Trainer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Fine-tuning\n",
    "\n",
    "Train each model variant sequentially, ensuring that progress is logged and checkpoints are saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, optimizer, training_args, save_path, tokenizer):\n",
    "    \"\"\"\n",
    "    Trains the given model using the provided training data loader and optimizer.\n",
    "    \"\"\"\n",
    "    # initialize accelerator for distributed training\n",
    "    accelerator = Accelerator(\n",
    "        mixed_precision=\"fp16\" if training_args.fp16 else None,\n",
    "        gradient_accumulation_steps=training_args.gradient_accumulation_steps\n",
    "    )\n",
    "    device = accelerator.device\n",
    "\n",
    "    # create scheduler for warmup\n",
    "    total_steps = len(train_loader) * training_args.num_train_epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=training_args.warmup_steps,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    # prepare model, optimizer, scheduler and dataloader\n",
    "    model, optimizer, scheduler, train_loader = accelerator.prepare(\n",
    "        model, optimizer, scheduler, train_loader\n",
    "    )\n",
    "    model.gradient_checkpointing_enable() # for memory optimization\n",
    "    model.config.use_cache = False # disable for compatibility with gradient checkpointing\n",
    "\n",
    "    # create scaler for mixed precision\n",
    "    scaler = torch.amp.GradScaler()\n",
    "\n",
    "    # clear memory\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    saved_checkpoints = []\n",
    "    model.train() # set model to training mode\n",
    "\n",
    "    for epoch in range(int(training_args.num_train_epochs)):\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{int(training_args.num_train_epochs)}\")\n",
    "\n",
    "        for step, batch in enumerate(progress_bar):\n",
    "            # accumulation step tracking\n",
    "            is_accumulation_step = (step + 1) % training_args.gradient_accumulation_steps != 0\n",
    "\n",
    "            # move batch to device\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            # forward pass with autocasting\n",
    "            with torch.amp.autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu', dtype=torch.float16):\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss / training_args.gradient_accumulation_steps\n",
    "\n",
    "            # backward pass with scaling\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # only step if we're at accumulation boundary\n",
    "            if not is_accumulation_step:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # logging based on TrainingArguments\n",
    "                if step % training_args.logging_steps == 0:\n",
    "                    logger.info(\n",
    "                        f\"Epoch {epoch}, Step {step}: Loss = {loss.item() * training_args.gradient_accumulation_steps}\"\n",
    "                    )\n",
    "\n",
    "                # save checkpoints based on strategy on main process only\n",
    "                if (training_args.save_strategy == \"steps\" and \n",
    "                    step % training_args.save_steps == 0):\n",
    "                    checkpoint_path = os.path.join(\n",
    "                        training_args.output_dir,\n",
    "                        f\"checkpoint-{epoch}-{step}\"\n",
    "                    )\n",
    "                    if accelerator.is_main_process:\n",
    "                        unwrapped_model = accelerator.unwrap_model(model)\n",
    "                        unwrapped_model.save_pretrained(checkpoint_path)\n",
    "                        tokenizer.save_pretrained(checkpoint_path)\n",
    "\n",
    "                        # manage checkpoint limit\n",
    "                        saved_checkpoints.append(checkpoint_path)\n",
    "                        if (len(saved_checkpoints) > training_args.save_total_limit \n",
    "                            and training_args.save_total_limit > 0):\n",
    "                            shutil.rmtree(saved_checkpoints.pop(0))\n",
    "\n",
    "            total_loss += loss.item() * training_args.gradient_accumulation_steps\n",
    "            progress_bar.set_postfix({\"Loss\": loss.item() * training_args.gradient_accumulation_steps})\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch + 1} Loss: {avg_loss}\")\n",
    "\n",
    "        # save best model\n",
    "        if training_args.load_best_model_at_end and avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            if accelerator.is_main_process:\n",
    "                best_model_path = os.path.join(training_args.output_dir, \"best_model\")\n",
    "                unwrapped_model = accelerator.unwrap_model(model)\n",
    "                unwrapped_model.save_pretrained(best_model_path)\n",
    "                tokenizer.save_pretrained(best_model_path)\n",
    "\n",
    "    print(f\"Training completed. Model saved to {save_path}\")\n",
    "\n",
    "\n",
    "# training loop to sequentially finetune buyer, seller, and generalist models\n",
    "for model_type, loader, save_dir, args in [\n",
    "    (\"buyer\", buyer_train_loader, buyer_finetuned_dir, buyer_training_args),\n",
    "    (\"seller\", seller_train_loader, seller_finetuned_dir, seller_training_args),\n",
    "    (\"generalist\", generalist_train_loader, generalist_finetuned_dir, generalist_training_args)\n",
    "]:\n",
    "    print(f\"\\nTraining {model_type} model...\")\n",
    "    model_copy = deepcopy(base_model).to(device).to(torch.float32)\n",
    "\n",
    "    optimizer = AdamW(\n",
    "        model_copy.parameters(),\n",
    "        lr=args.learning_rate,\n",
    "        weight_decay=args.weight_decay\n",
    "    )\n",
    "\n",
    "    train_model(\n",
    "        model=model_copy,\n",
    "        train_loader=loader,\n",
    "        optimizer=optimizer,\n",
    "        training_args=args,\n",
    "        save_path=save_dir,\n",
    "        tokenizer=base_tokenizer\n",
    "    )\n",
    "\n",
    "    # clear memory\n",
    "    del model_copy, optimizer\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Finetuned Models Evaluation\n",
    "\n",
    "Ensure that each finetuned model is saved correctly in its designated directory with all necessary files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_model_save(save_path, model_name):\n",
    "    \"\"\"\n",
    "    Verifies that the finetuned model is saved correctly by checking for essential files.\n",
    "\n",
    "    Args:\n",
    "        save_path (str): Directory path where the model is saved.\n",
    "        model_name (str): Name identifier for logging purposes.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If essential model files are missing.\n",
    "    \"\"\"\n",
    "    essential_files = ['config.json', 'pytorch_model.bin', 'tokenizer.json', 'tokenizer_config.json']\n",
    "    for file in essential_files:\n",
    "        if not os.path.exists(os.path.join(save_path, file)):\n",
    "            logger.error(f\"Missing {file} in {model_name} at {save_path}\")\n",
    "            raise FileNotFoundError(f\"Missing {file} in {model_name} at {save_path}\")\n",
    "    logger.info(f\"All essential files found for {model_name} at {save_path}\")\n",
    "\n",
    "# verify buyer, seller, and generalist specialist models\n",
    "verify_model_save(buyer_finetuned_dir, \"Buyer Specialist\")\n",
    "verify_model_save(seller_finetuned_dir, \"Seller Specialist\")\n",
    "verify_model_save(generalist_finetuned_dir, \"Generalist Negotiator\")\n",
    "logger.info(\"All finetuned models have been successfully saved and verified.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do a quick evaluation of the finetuned models to ensure that they are functioning as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, val_loader):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the validation dataset using DataLoader.\n",
    "\n",
    "    Args:\n",
    "        model: Trained model to evaluate.\n",
    "        val_loader: DataLoader for validation data.\n",
    "\n",
    "    Returns:\n",
    "        Perplexity of the model on the validation dataset.\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "            batch = {key: val.to(device) for key, val in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss.item()\n",
    "            total_loss += loss * batch['input_ids'].size(0)\n",
    "            total_tokens += batch['input_ids'].numel()\n",
    "\n",
    "    # calculate average loss and perplexity\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss))\n",
    "    return perplexity.item()\n",
    "\n",
    "\n",
    "# load, evaluate, and remove buyer model\n",
    "buyer_model = model_loader.load_model(buyer_finetuned_dir)\n",
    "buyer_perplexity = evaluate_model(buyer_model, buyer_val_loader)\n",
    "logger.info(f\"Buyer Specialist Perplexity: {buyer_perplexity}\")\n",
    "del buyer_model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# load, evaluate, and remove seller model\n",
    "seller_model = model_loader.load_model(seller_finetuned_dir)\n",
    "seller_perplexity = evaluate_model(seller_model, seller_val_loader)\n",
    "logger.info(f\"Seller Specialist Perplexity: {seller_perplexity}\")\n",
    "del seller_model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# load, evaluate, and remove generalist model\n",
    "generalist_model = model_loader.load_model(generalist_finetuned_dir)\n",
    "generalist_perplexity = evaluate_model(generalist_model, generalist_val_loader)\n",
    "logger.info(f\"Generalist Negotiator Perplexity: {generalist_perplexity}\")\n",
    "del generalist_model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate sample responses from each model variant to verify role-specific behaviors and negotiation strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, tokenizer, prompt, max_tokens=50):\n",
    "    \"\"\"\n",
    "    Generates a response from the model based on the provided prompt.\n",
    "\n",
    "    Args:\n",
    "        model (AutoModelForCausalLM): The finetuned model.\n",
    "        tokenizer (AutoTokenizer): The corresponding tokenizer.\n",
    "        prompt (str): The input prompt for the model.\n",
    "        max_tokens (int): Maximum number of tokens to generate.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated response.\n",
    "    \"\"\"\n",
    "    model = model.to(device) # move model to device\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_tokens,\n",
    "        do_sample=True, # use sampling\n",
    "        temperature=0.7, # control randomness\n",
    "        top_p=0.9, # nucleus sampling\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "\n",
    "# define sample prompts\n",
    "buyer_prompt = (\n",
    "    \"You are a buyer negotiating over items.\\n\"\n",
    "    \"Analyze the situation and determine if you should accept, reject, or counteroffer.\\n\"\n",
    "    \"If you counteroffer, provide a new price for the item.\\n\"\n",
    "    \"Your item values: {'book': {'count':1, 'value':10}}\\n\"\n",
    "    \"Partner's values: {'book': {'count':1, 'value':8}}\\n\"\n",
    "    \"Previous messages:\\n\"\n",
    "    \"Seller: I can offer the book for $15.\\n\"\n",
    "    \"Your response:\"\n",
    ")\n",
    "seller_prompt = (\n",
    "    \"You are a seller negotiating over items.\\n\"\n",
    "    \"Analyze the situation and determine if you should accept, reject, or counteroffer.\\n\"\n",
    "    \"If you counteroffer, provide a new price for the item.\\n\"\n",
    "    \"Your item values: {'book': {'count':1, 'value':10}}\\n\"\n",
    "    \"Partner's values: {'book': {'count':1, 'value':8}}\\n\"\n",
    "    \"Previous messages:\\n\"\n",
    "    \"Buyer: I can offer $5 for the book.\\n\"\n",
    "    \"Your response:\"\n",
    ")\n",
    "generalist_buyer_prompt = (\n",
    "    \"You are a buyer negotiating over items.\\n\"\n",
    "    \"Analyze the situation and determine if you should accept, reject, or counteroffer.\\n\"\n",
    "    \"If you counteroffer, provide a new price for the item.\\n\"\n",
    "    \"Your item values: {'book': {'count':1, 'value':10}}\\n\"\n",
    "    \"Partner's values: {'book': {'count':1, 'value':8}}\\n\"\n",
    "    \"Previous messages:\\n\"\n",
    "    \"Seller: I can offer the book for $15.\\n\"\n",
    "    \"Your response:\"\n",
    ")\n",
    "generalist_seller_prompt = (\n",
    "    \"You are a seller negotiating over items.\\n\"\n",
    "    \"Analyze the situation and determine if you should accept, reject, or counteroffer.\\n\"\n",
    "    \"If you counteroffer, provide a new price for the item.\\n\"\n",
    "    \"Your item values: {'book': {'count':1, 'value':10}}\\n\"\n",
    "    \"Partner's values: {'book': {'count':1, 'value':8}}\\n\"\n",
    "    \"Previous messages:\\n\"\n",
    "    \"Buyer: I can offer $5 for the book.\\n\"\n",
    "    \"Your response:\"\n",
    ")\n",
    "\n",
    "\n",
    "# load, generate response, and remove buyer model\n",
    "buyer_model = model_loader.reload_model(buyer_finetuned_dir)\n",
    "logger.info(\"Loaded finetuned buyer model\")\n",
    "buyer_response = generate_response(buyer_model, base_tokenizer, buyer_prompt)\n",
    "print(f\"Buyer Specialist Prompt:\\n{buyer_prompt}\\n\")\n",
    "print(f\"Buyer Specialist Response:\\n{buyer_response}\\n\")\n",
    "del buyer_model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# load, generate response, and remove seller model\n",
    "seller_model = model_loader.reload_model(seller_finetuned_dir)\n",
    "logger.info(\"Loaded finetuned seller model\")\n",
    "seller_response = generate_response(seller_model, base_tokenizer, seller_prompt)\n",
    "print(f\"Seller Specialist Prompt:\\n{seller_prompt}\\n\")\n",
    "print(f\"Seller Specialist Response:\\n{seller_response}\\n\")\n",
    "del seller_model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# load, generate response, and remove generalist model for buyer prompt\n",
    "generalist_model = model_loader.reload_model(generalist_finetuned_dir)\n",
    "logger.info(\"Loaded finetuned generalist model\")\n",
    "generalist_buyer_response = generate_response(generalist_model, base_tokenizer, generalist_buyer_prompt)\n",
    "print(f\"Generalist Negotiator (Buyer) Prompt:\\n{generalist_buyer_prompt}\\n\")\n",
    "print(f\"Generalist Negotiator (Buyer) Response:\\n{generalist_buyer_response}\\n\")\n",
    "del generalist_model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# load, generate response, and remove generalist model for seller prompt\n",
    "generalist_model = model_loader.reload_model(generalist_finetuned_dir)\n",
    "generalist_seller_response = generate_response(generalist_model, base_tokenizer, generalist_seller_prompt)\n",
    "print(f\"Generalist Negotiator (Seller) Prompt:\\n{generalist_seller_prompt}\\n\")\n",
    "print(f\"Generalist Negotiator (Seller) Response:\\n{generalist_seller_response}\\n\")\n",
    "del generalist_model\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

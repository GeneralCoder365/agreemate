{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AgreeMate Finetuner Notebook\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [Setup and Initialization](#setup-and-initialization)\n",
    "3. [Data Loading and Preparation](#data-loading-and-preparation)\n",
    "4. [Model Loading](#model-loading)\n",
    "5. [Dataset Creation](#dataset-creation)\n",
    "6. [Training Configuration](#training-configuration)\n",
    "7. [Model Fine-tuning](#model-fine-tuning)\n",
    "8. [Finetuned Models Evaluation](#evaluate-finetuned-models)\n",
    "\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "Welcome to the **AgreeMate Finetuner Notebook**. This notebook is designed to fine-tune the **Llama-3.2-1B-Instruct** model for specialized negotiation roles within bargaining environments. The objective is to create three distinct model variants:\n",
    "\n",
    "1. **Buyer Specialist**: Optimized for buyer-specific negotiation strategies.\n",
    "2. **Seller Specialist**: Optimized for seller-specific negotiation strategies.\n",
    "3. **Generalist Negotiator**: Capable of handling both buyer and seller roles effectively.\n",
    "\n",
    "This finetuning process leverages existing modules (`model_loader.py` and `data_loader.py`) to ensure seamless integration and efficient path management within the Jupyter environment. Additionally, the notebook incorporates robust checkpointing mechanisms to prevent loss of progress in case of unexpected failures.\n",
    "\n",
    "## 2. Setup and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting AgreeMate Finetuner Notebook\n",
      "INFO:__main__:Training on cuda\n",
      "INFO:__main__:Finetuned models will be saved to: c:\\Tocho\\umd\\fall_2024\\CMSC723\\agreemate\\finetuning\n"
     ]
    }
   ],
   "source": [
    "import os, logging, shutil, torch\n",
    "from copy import deepcopy\n",
    "from logging.handlers import RotatingFileHandler\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import TrainingArguments, get_linear_schedule_with_warmup\n",
    "from accelerate import Accelerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128,expandable_segments:True\"\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from data_loader import NegotiationDialogueDataLoader\n",
    "from model_loader import ModelLoader\n",
    "\n",
    "\n",
    "# configure logging\n",
    "handler = RotatingFileHandler('./progress/finetuner.log', maxBytes=10**6, backupCount=5)\n",
    "logging.basicConfig(\n",
    "    handlers=[handler], # limit log file size to 5MB\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    level=logging.DEBUG\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.info(\"Starting AgreeMate Finetuner Notebook\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logger.info(f\"Training on {device}\")\n",
    "\n",
    "\n",
    "# define directories\n",
    "finetuning_dir = os.getcwd()\n",
    "pretrained_dir = os.path.join(finetuning_dir, 'models--meta-llama--Llama-3.2-1B-Instruct')\n",
    "buyer_finetuned_dir = os.path.join(finetuning_dir, 'models--buyer-finetuned--Llama-3.2-1B-Instruct')\n",
    "seller_finetuned_dir = os.path.join(finetuning_dir, 'models--seller-finetuned--Llama-3.2-1B-Instruct')\n",
    "generalist_finetuned_dir = os.path.join(finetuning_dir, 'models--generalist-finetuned--Llama-3.2-1B-Instruct')\n",
    "os.makedirs(buyer_finetuned_dir, exist_ok=True)\n",
    "os.makedirs(seller_finetuned_dir, exist_ok=True)\n",
    "os.makedirs(generalist_finetuned_dir, exist_ok=True)\n",
    "logger.info(f\"Finetuned models will be saved to: {finetuning_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Preparation\n",
    "\n",
    "Use the existing `NegotiationDialogueDataLoader` to load and prepare the datasets for finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Initialized NegotiationDialogueDataLoader\n",
      "INFO:data_loader:Loading buyer dataset from c:\\Tocho\\umd\\fall_2024\\CMSC723\\agreemate\\data\\deal_or_no_deal\\buyer_training.csv\n",
      "INFO:data_loader:Loaded 36596 examples\n",
      "INFO:data_loader:Loading seller dataset from c:\\Tocho\\umd\\fall_2024\\CMSC723\\agreemate\\data\\deal_or_no_deal\\seller_training.csv\n",
      "INFO:data_loader:Loaded 36595 examples\n",
      "INFO:data_loader:Loading generalist dataset from c:\\Tocho\\umd\\fall_2024\\CMSC723\\agreemate\\data\\deal_or_no_deal\\generalist_training.csv\n",
      "INFO:data_loader:Loaded 73191 examples\n",
      "INFO:__main__:Loaded buyer dataset with 36596 examples\n",
      "INFO:__main__:Loaded seller dataset with 36595 examples\n",
      "INFO:__main__:Loaded generalist dataset with 73191 examples\n"
     ]
    }
   ],
   "source": [
    "data_loader = NegotiationDialogueDataLoader()\n",
    "logger.info(\"Initialized NegotiationDialogueDataLoader\")\n",
    "\n",
    "# load datasets\n",
    "buyer_df = data_loader.load_dataset(\"buyer\")\n",
    "seller_df = data_loader.load_dataset(\"seller\")\n",
    "generalist_df = data_loader.load_dataset(\"generalist\")\n",
    "logger.info(f\"Loaded buyer dataset with {len(buyer_df)} examples\")\n",
    "logger.info(f\"Loaded seller dataset with {len(seller_df)} examples\")\n",
    "logger.info(f\"Loaded generalist dataset with {len(generalist_df)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Finetuning Data\n",
    "\n",
    "Format the loaded data into input-target pairs suitable for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Prepared training data for buyer, seller, and generalist models\n"
     ]
    }
   ],
   "source": [
    "buyer_data = data_loader.prepare_for_training(buyer_df)\n",
    "seller_data = data_loader.prepare_for_training(seller_df)\n",
    "generalist_data = data_loader.prepare_for_training(generalist_df)\n",
    "logger.info(\"Prepared training data for buyer, seller, and generalist models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Loading\n",
    "\n",
    "Use the provided `ModelLoader` to handle model loading and caching.\n",
    "\n",
    "Load the base Llama-3.2-1B-Instruct model, which will be finetuned for each specific role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Initialized ModelLoader\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cache directory: c:\\Tocho\\umd\\fall_2024\\CMSC723\\agreemate\\finetuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /meta-llama/Llama-3.2-1B-Instruct/resolve/main/tokenizer_config.json HTTP/11\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json HTTP/11\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Tokenizer loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ainesh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\accelerate\\utils\\modeling.py:329: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  new_value = value.to(device)\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /meta-llama/Llama-3.2-1B-Instruct/resolve/main/generation_config.json HTTP/11\" 200 0\n",
      "INFO:__main__:Loaded base Llama-3.2-1B-Instruct model and tokenizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "model_loader = ModelLoader()\n",
    "logger.info(\"Initialized ModelLoader\")\n",
    "\n",
    "# load base model and tokenizer\n",
    "base_model, base_tokenizer = model_loader.load_model_and_tokenizer()\n",
    "if base_tokenizer.pad_token_id is None: # set pad token to eos token\n",
    "    base_tokenizer.pad_token = base_tokenizer.eos_token\n",
    "    base_tokenizer.pad_token_id = base_tokenizer.eos_token_id\n",
    "torch.cuda.empty_cache() # clear any residual memory\n",
    "torch.cuda.reset_peak_memory_stats() # reset memory tracking stats\n",
    "logger.info(\"Loaded base Llama-3.2-1B-Instruct model and tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dataset Creation\n",
    "\n",
    "### Define Custom Dataset Class\n",
    "\n",
    "Define a `NegotiationDataset` class to interface with Hugging Face's `Trainer`. This class handles the encoding of inputs and targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegotiationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for Negotiation Finetuning.\n",
    "\n",
    "    Attributes:\n",
    "        encodings (Dict): Tokenized inputs.\n",
    "        labels (List[List[int]]): Tokenized target responses.\n",
    "    \"\"\"\n",
    "    def __init__(self, encodings, labels):\n",
    "        \"\"\"\n",
    "        Initializes the dataset with encodings and labels.\n",
    "\n",
    "        Args:\n",
    "            encodings (Dict): Tokenized input data.\n",
    "            labels (List[List[int]]): Tokenized target responses.\n",
    "        \"\"\"\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves the input-target pair at the specified index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the data point.\n",
    "\n",
    "        Returns:\n",
    "            Dict: A dictionary containing input_ids, attention_mask, and labels.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'input_ids': self.encodings['input_ids'][idx].clone().detach(),\n",
    "            'attention_mask': self.encodings['attention_mask'][idx].clone().detach(),\n",
    "            'labels': self.labels[idx].clone().detach()\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of examples in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: Number of examples.\n",
    "        \"\"\"\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data and Tokenize Inputs and Targets\n",
    "\n",
    "Split the prepared data into training and validation sets at a 90-10 ratio.\n",
    "Tokenize the prepared data using the respective tokenizers for each model variant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Created DataLoaders for buyer, seller, and generalist models\n"
     ]
    }
   ],
   "source": [
    "def split_dataset(data, test_size=0.1):\n",
    "    \"\"\"Splits the dataset into training and validation sets.\"\"\"\n",
    "    inputs_train, inputs_val, labels_train, labels_val = train_test_split(\n",
    "        data['input'], data['target'], test_size=test_size, random_state=42\n",
    "    )\n",
    "    return inputs_train, inputs_val, labels_train, labels_val\n",
    "\n",
    "def tokenize_data(inputs, targets, tokenizer):\n",
    "    \"\"\"Tokenizes input and target data by concatenating them and setting labels to -100 for input tokens.\"\"\"\n",
    "\n",
    "    # concatenate inputs and targets with a separator (e.g., space or special token if needed)\n",
    "    concatenated = [f\"{inp} {tgt}\" for inp, tgt in zip(inputs, targets)]\n",
    "\n",
    "    # tokenize the concatenated sequences\n",
    "    encodings = tokenizer(\n",
    "        concatenated,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    # tokenize inputs separately to determine the boundary between input and target tokens\n",
    "    input_encodings = tokenizer(\n",
    "        inputs,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    # initialize labels with -100 (representing tokens to be ignored in loss computation)\n",
    "    labels = torch.full_like(encodings['input_ids'], -100)\n",
    "\n",
    "    # set labels for target tokens\n",
    "    for i in range(len(inputs)):\n",
    "        input_len = input_encodings['input_ids'][i].ne(tokenizer.pad_token_id).sum().item()\n",
    "        # ensure we don't exceed the sequence length\n",
    "        target_len = encodings['input_ids'][i].ne(tokenizer.pad_token_id).sum().item() - input_len\n",
    "        if target_len > 0:\n",
    "            labels[i, input_len:input_len + target_len] = encodings['input_ids'][i, input_len:input_len + target_len]\n",
    "\n",
    "    return NegotiationDataset(encodings, labels)\n",
    "\n",
    "def create_dataloaders(train_dataset, val_dataset, batch_size):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "# buyer, seller, and generalist dataset splitting\n",
    "buyer_inputs_train, buyer_inputs_val, buyer_labels_train, buyer_labels_val = split_dataset(buyer_data)\n",
    "seller_inputs_train, seller_inputs_val, seller_labels_train, seller_labels_val = split_dataset(seller_data)\n",
    "generalist_inputs_train, generalist_inputs_val, generalist_labels_train, generalist_labels_val = split_dataset(generalist_data)\n",
    "\n",
    "# tokenize datasets\n",
    "buyer_train_dataset = tokenize_data(buyer_inputs_train, buyer_labels_train, base_tokenizer)\n",
    "buyer_val_dataset = tokenize_data(buyer_inputs_val, buyer_labels_val, base_tokenizer)\n",
    "seller_train_dataset = tokenize_data(seller_inputs_train, seller_labels_train, base_tokenizer)\n",
    "seller_val_dataset = tokenize_data(seller_inputs_val, seller_labels_val, base_tokenizer)\n",
    "generalist_train_dataset = tokenize_data(generalist_inputs_train, generalist_labels_train, base_tokenizer)\n",
    "generalist_val_dataset = tokenize_data(generalist_inputs_val, generalist_labels_val, base_tokenizer)\n",
    "\n",
    "# create dataloaders\n",
    "batch_size = 1\n",
    "buyer_train_loader, buyer_val_loader = create_dataloaders(buyer_train_dataset, buyer_val_dataset, batch_size)\n",
    "seller_train_loader, seller_val_loader = create_dataloaders(seller_train_dataset, seller_val_dataset, batch_size)\n",
    "generalist_train_loader, generalist_val_loader = create_dataloaders(generalist_train_dataset, generalist_val_dataset, batch_size)\n",
    "\n",
    "logger.info(\"Created DataLoaders for buyer, seller, and generalist models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Configuration\n",
    "\n",
    "Set up the training parameters, including learning rate, batch size, number of epochs, and checkpointing strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Defined TrainingArguments for Trainer\n"
     ]
    }
   ],
   "source": [
    "# base training arguments\n",
    "base_training_args = {\n",
    "    \"num_train_epochs\": 2,               # number of training epochs\n",
    "    \"per_device_train_batch_size\": 1,    # batch size per device during training\n",
    "    \"per_device_eval_batch_size\": 1,     # batch size for evaluation\n",
    "    \"gradient_accumulation_steps\": 16,    # accumulates gradients over 8 steps, simulating batch size of 1*16=16\n",
    "    \"fp16\": True,                        # use mixed precision training to reduce memory usage\n",
    "    \"warmup_steps\": 500,                 # number of warmup steps for learning rate scheduler\n",
    "    \"learning_rate\": 5e-5,               # learning rate for optimizer\n",
    "    \"weight_decay\": 0.01,                # weight decay for optimizer\n",
    "    \"logging_steps\": 10,                 # log every 10 steps\n",
    "    \"eval_strategy\": \"steps\",            # evaluate every few steps\n",
    "    \"save_strategy\": \"steps\",            # save model every few steps (CHECKPOINTS)\n",
    "    \"save_steps\": 100,                   # save checkpoint every 100 steps\n",
    "    \"load_best_model_at_end\": True,      # load/save the best model when finished training (also works with CHECKPOINTS)\n",
    "    \"save_total_limit\": 3,               # only keep the last 3 checkpoints to save disk space\n",
    "    \"report_to\": \"none\"                  # disable reporting to WandB or other services\n",
    "}\n",
    "\n",
    "# buyer, seller, generalist output, logging specifications\n",
    "buyer_training_args = TrainingArguments(\n",
    "    output_dir=os.path.join(finetuning_dir, 'progress', 'buyer'),\n",
    "    logging_dir=os.path.join(finetuning_dir, 'progress', 'buyer', 'logs'),\n",
    "    **base_training_args\n",
    ")\n",
    "seller_training_args = TrainingArguments(\n",
    "    output_dir=os.path.join(finetuning_dir, 'progress', 'seller'),\n",
    "    logging_dir=os.path.join(finetuning_dir, 'progress', 'seller', 'logs'),\n",
    "    **base_training_args\n",
    ")\n",
    "generalist_training_args = TrainingArguments(\n",
    "    output_dir=os.path.join(finetuning_dir, 'progress', 'generalist'),\n",
    "    logging_dir=os.path.join(finetuning_dir, 'progress', 'generalist', 'logs'),\n",
    "    **base_training_args\n",
    ")\n",
    "\n",
    "logger.info(\"Defined TrainingArguments for Trainer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Fine-tuning\n",
    "\n",
    "Train each model variant sequentially, ensuring that progress is logged and checkpoints are saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training buyer model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f2b9544d69748b88a20b335bced5337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/2:   0%|          | 0/32936 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free. Of the allocated memory 11.76 GiB is allocated by PyTorch, and 201.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 121\u001b[0m\n\u001b[0;32m    113\u001b[0m model_copy \u001b[38;5;241m=\u001b[39m deepcopy(base_model)\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m    115\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m AdamW(\n\u001b[0;32m    116\u001b[0m     model_copy\u001b[38;5;241m.\u001b[39mparameters(),\n\u001b[0;32m    117\u001b[0m     lr\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mlearning_rate,\n\u001b[0;32m    118\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mweight_decay\n\u001b[0;32m    119\u001b[0m )\n\u001b[1;32m--> 121\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_copy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_tokenizer\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;66;03m# clear memory\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m model_copy, optimizer\n",
      "Cell \u001b[1;32mIn[8], line 55\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, optimizer, training_args, save_path, tokenizer)\u001b[0m\n\u001b[0;32m     52\u001b[0m     loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;241m/\u001b[39m training_args\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# backward pass with scaling\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# only step if we're at accumulation boundary\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accumulation_step:\n",
      "File \u001b[1;32mc:\\Users\\ainesh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ainesh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ainesh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free. Of the allocated memory 11.76 GiB is allocated by PyTorch, and 201.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, optimizer, training_args, save_path, tokenizer):\n",
    "    \"\"\"\n",
    "    Trains the given model using the provided training data loader and optimizer.\n",
    "    \"\"\"\n",
    "    # initialize accelerator for distributed training\n",
    "    accelerator = Accelerator(\n",
    "        mixed_precision=\"fp16\" if training_args.fp16 else None,\n",
    "        gradient_accumulation_steps=training_args.gradient_accumulation_steps\n",
    "    )\n",
    "    device = accelerator.device\n",
    "\n",
    "    # create scheduler for warmup\n",
    "    total_steps = len(train_loader) * training_args.num_train_epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=training_args.warmup_steps,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    # prepare model, optimizer, scheduler and dataloader\n",
    "    model, optimizer, scheduler, train_loader = accelerator.prepare(\n",
    "        model, optimizer, scheduler, train_loader\n",
    "    )\n",
    "    model.gradient_checkpointing_enable() # for memory optimization\n",
    "    model.config.use_cache = False # disable for compatibility with gradient checkpointing\n",
    "\n",
    "    # create scaler for mixed precision\n",
    "    scaler = torch.amp.GradScaler()\n",
    "\n",
    "    # clear memory\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    saved_checkpoints = []\n",
    "    model.train() # set model to training mode\n",
    "\n",
    "    for epoch in range(int(training_args.num_train_epochs)):\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{int(training_args.num_train_epochs)}\")\n",
    "\n",
    "        for step, batch in enumerate(progress_bar):\n",
    "            # accumulation step tracking\n",
    "            is_accumulation_step = (step + 1) % training_args.gradient_accumulation_steps != 0\n",
    "\n",
    "            # move batch to device\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            # forward pass with autocasting\n",
    "            with torch.amp.autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu', dtype=torch.float16):\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss / training_args.gradient_accumulation_steps\n",
    "\n",
    "            # backward pass with scaling\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # only step if we're at accumulation boundary\n",
    "            if not is_accumulation_step:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # logging based on TrainingArguments\n",
    "                if step % training_args.logging_steps == 0:\n",
    "                    logger.info(\n",
    "                        f\"Epoch {epoch}, Step {step}: Loss = {loss.item() * training_args.gradient_accumulation_steps}\"\n",
    "                    )\n",
    "\n",
    "                # save checkpoints based on strategy on main process only\n",
    "                if (training_args.save_strategy == \"steps\" and \n",
    "                    step % training_args.save_steps == 0):\n",
    "                    checkpoint_path = os.path.join(\n",
    "                        training_args.output_dir,\n",
    "                        f\"checkpoint-{epoch}-{step}\"\n",
    "                    )\n",
    "                    if accelerator.is_main_process:\n",
    "                        unwrapped_model = accelerator.unwrap_model(model)\n",
    "                        unwrapped_model.save_pretrained(checkpoint_path)\n",
    "                        tokenizer.save_pretrained(checkpoint_path)\n",
    "\n",
    "                        # manage checkpoint limit\n",
    "                        saved_checkpoints.append(checkpoint_path)\n",
    "                        if (len(saved_checkpoints) > training_args.save_total_limit \n",
    "                            and training_args.save_total_limit > 0):\n",
    "                            shutil.rmtree(saved_checkpoints.pop(0))\n",
    "\n",
    "            total_loss += loss.item() * training_args.gradient_accumulation_steps\n",
    "            progress_bar.set_postfix({\"Loss\": loss.item() * training_args.gradient_accumulation_steps})\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch + 1} Loss: {avg_loss}\")\n",
    "\n",
    "        # save best model\n",
    "        if training_args.load_best_model_at_end and avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            if accelerator.is_main_process:\n",
    "                best_model_path = os.path.join(training_args.output_dir, \"best_model\")\n",
    "                unwrapped_model = accelerator.unwrap_model(model)\n",
    "                unwrapped_model.save_pretrained(best_model_path)\n",
    "                tokenizer.save_pretrained(best_model_path)\n",
    "\n",
    "    print(f\"Training completed. Model saved to {save_path}\")\n",
    "\n",
    "\n",
    "# training loop to sequentially finetune buyer, seller, and generalist models\n",
    "for model_type, loader, save_dir, args in [\n",
    "    (\"buyer\", buyer_train_loader, buyer_finetuned_dir, buyer_training_args),\n",
    "    (\"seller\", seller_train_loader, seller_finetuned_dir, seller_training_args),\n",
    "    (\"generalist\", generalist_train_loader, generalist_finetuned_dir, generalist_training_args)\n",
    "]:\n",
    "    print(f\"\\nTraining {model_type} model...\")\n",
    "    model_copy = deepcopy(base_model).to(device).to(torch.float32)\n",
    "\n",
    "    optimizer = AdamW(\n",
    "        model_copy.parameters(),\n",
    "        lr=args.learning_rate,\n",
    "        weight_decay=args.weight_decay\n",
    "    )\n",
    "\n",
    "    train_model(\n",
    "        model=model_copy,\n",
    "        train_loader=loader,\n",
    "        optimizer=optimizer,\n",
    "        training_args=args,\n",
    "        save_path=save_dir,\n",
    "        tokenizer=base_tokenizer\n",
    "    )\n",
    "\n",
    "    # clear memory\n",
    "    del model_copy, optimizer\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Finetuned Models Evaluation\n",
    "\n",
    "Ensure that each finetuned model is saved correctly in its designated directory with all necessary files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_model_save(save_path, model_name):\n",
    "    \"\"\"\n",
    "    Verifies that the finetuned model is saved correctly by checking for essential files.\n",
    "\n",
    "    Args:\n",
    "        save_path (str): Directory path where the model is saved.\n",
    "        model_name (str): Name identifier for logging purposes.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If essential model files are missing.\n",
    "    \"\"\"\n",
    "    essential_files = ['config.json', 'pytorch_model.bin', 'tokenizer.json', 'tokenizer_config.json']\n",
    "    for file in essential_files:\n",
    "        if not os.path.exists(os.path.join(save_path, file)):\n",
    "            logger.error(f\"Missing {file} in {model_name} at {save_path}\")\n",
    "            raise FileNotFoundError(f\"Missing {file} in {model_name} at {save_path}\")\n",
    "    logger.info(f\"All essential files found for {model_name} at {save_path}\")\n",
    "\n",
    "# verify buyer, seller, and generalist specialist models\n",
    "verify_model_save(buyer_finetuned_dir, \"Buyer Specialist\")\n",
    "verify_model_save(seller_finetuned_dir, \"Seller Specialist\")\n",
    "verify_model_save(generalist_finetuned_dir, \"Generalist Negotiator\")\n",
    "logger.info(\"All finetuned models have been successfully saved and verified.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do a quick evaluation of the finetuned models to ensure that they are functioning as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, val_loader):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the validation dataset using DataLoader.\n",
    "\n",
    "    Args:\n",
    "        model: Trained model to evaluate.\n",
    "        val_loader: DataLoader for validation data.\n",
    "\n",
    "    Returns:\n",
    "        Perplexity of the model on the validation dataset.\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "            batch = {key: val.to(device) for key, val in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss.item()\n",
    "            total_loss += loss * batch['input_ids'].size(0)\n",
    "            total_tokens += batch['input_ids'].numel()\n",
    "\n",
    "    # calculate average loss and perplexity\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss))\n",
    "    return perplexity.item()\n",
    "\n",
    "\n",
    "# load, evaluate, and remove buyer model\n",
    "buyer_model = model_loader.load_model(buyer_finetuned_dir)\n",
    "buyer_perplexity = evaluate_model(buyer_model, buyer_val_loader)\n",
    "logger.info(f\"Buyer Specialist Perplexity: {buyer_perplexity}\")\n",
    "del buyer_model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# load, evaluate, and remove seller model\n",
    "seller_model = model_loader.load_model(seller_finetuned_dir)\n",
    "seller_perplexity = evaluate_model(seller_model, seller_val_loader)\n",
    "logger.info(f\"Seller Specialist Perplexity: {seller_perplexity}\")\n",
    "del seller_model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# load, evaluate, and remove generalist model\n",
    "generalist_model = model_loader.load_model(generalist_finetuned_dir)\n",
    "generalist_perplexity = evaluate_model(generalist_model, generalist_val_loader)\n",
    "logger.info(f\"Generalist Negotiator Perplexity: {generalist_perplexity}\")\n",
    "del generalist_model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate sample responses from each model variant to verify role-specific behaviors and negotiation strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, tokenizer, prompt, max_tokens=50):\n",
    "    \"\"\"\n",
    "    Generates a response from the model based on the provided prompt.\n",
    "\n",
    "    Args:\n",
    "        model (AutoModelForCausalLM): The finetuned model.\n",
    "        tokenizer (AutoTokenizer): The corresponding tokenizer.\n",
    "        prompt (str): The input prompt for the model.\n",
    "        max_tokens (int): Maximum number of tokens to generate.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated response.\n",
    "    \"\"\"\n",
    "    model = model.to(device) # move model to device\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_tokens,\n",
    "        do_sample=True, # use sampling\n",
    "        temperature=0.7, # control randomness\n",
    "        top_p=0.9, # nucleus sampling\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "\n",
    "# define sample prompts\n",
    "buyer_prompt = (\n",
    "    \"You are a buyer negotiating over items.\\n\"\n",
    "    \"Analyze the situation and determine if you should accept, reject, or counteroffer.\\n\"\n",
    "    \"If you counteroffer, provide a new price for the item.\\n\"\n",
    "    \"Your item values: {'book': {'count':1, 'value':10}}\\n\"\n",
    "    \"Partner's values: {'book': {'count':1, 'value':8}}\\n\"\n",
    "    \"Previous messages:\\n\"\n",
    "    \"Seller: I can offer the book for $15.\\n\"\n",
    "    \"Your response:\"\n",
    ")\n",
    "seller_prompt = (\n",
    "    \"You are a seller negotiating over items.\\n\"\n",
    "    \"Analyze the situation and determine if you should accept, reject, or counteroffer.\\n\"\n",
    "    \"If you counteroffer, provide a new price for the item.\\n\"\n",
    "    \"Your item values: {'book': {'count':1, 'value':10}}\\n\"\n",
    "    \"Partner's values: {'book': {'count':1, 'value':8}}\\n\"\n",
    "    \"Previous messages:\\n\"\n",
    "    \"Buyer: I can offer $5 for the book.\\n\"\n",
    "    \"Your response:\"\n",
    ")\n",
    "generalist_buyer_prompt = (\n",
    "    \"You are a buyer negotiating over items.\\n\"\n",
    "    \"Analyze the situation and determine if you should accept, reject, or counteroffer.\\n\"\n",
    "    \"If you counteroffer, provide a new price for the item.\\n\"\n",
    "    \"Your item values: {'book': {'count':1, 'value':10}}\\n\"\n",
    "    \"Partner's values: {'book': {'count':1, 'value':8}}\\n\"\n",
    "    \"Previous messages:\\n\"\n",
    "    \"Seller: I can offer the book for $15.\\n\"\n",
    "    \"Your response:\"\n",
    ")\n",
    "generalist_seller_prompt = (\n",
    "    \"You are a seller negotiating over items.\\n\"\n",
    "    \"Analyze the situation and determine if you should accept, reject, or counteroffer.\\n\"\n",
    "    \"If you counteroffer, provide a new price for the item.\\n\"\n",
    "    \"Your item values: {'book': {'count':1, 'value':10}}\\n\"\n",
    "    \"Partner's values: {'book': {'count':1, 'value':8}}\\n\"\n",
    "    \"Previous messages:\\n\"\n",
    "    \"Buyer: I can offer $5 for the book.\\n\"\n",
    "    \"Your response:\"\n",
    ")\n",
    "\n",
    "\n",
    "# load, generate response, and remove buyer model\n",
    "buyer_model = model_loader.reload_model(buyer_finetuned_dir)\n",
    "logger.info(\"Loaded finetuned buyer model\")\n",
    "buyer_response = generate_response(buyer_model, base_tokenizer, buyer_prompt)\n",
    "print(f\"Buyer Specialist Prompt:\\n{buyer_prompt}\\n\")\n",
    "print(f\"Buyer Specialist Response:\\n{buyer_response}\\n\")\n",
    "del buyer_model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# load, generate response, and remove seller model\n",
    "seller_model = model_loader.reload_model(seller_finetuned_dir)\n",
    "logger.info(\"Loaded finetuned seller model\")\n",
    "seller_response = generate_response(seller_model, base_tokenizer, seller_prompt)\n",
    "print(f\"Seller Specialist Prompt:\\n{seller_prompt}\\n\")\n",
    "print(f\"Seller Specialist Response:\\n{seller_response}\\n\")\n",
    "del seller_model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# load, generate response, and remove generalist model for buyer prompt\n",
    "generalist_model = model_loader.reload_model(generalist_finetuned_dir)\n",
    "logger.info(\"Loaded finetuned generalist model\")\n",
    "generalist_buyer_response = generate_response(generalist_model, base_tokenizer, generalist_buyer_prompt)\n",
    "print(f\"Generalist Negotiator (Buyer) Prompt:\\n{generalist_buyer_prompt}\\n\")\n",
    "print(f\"Generalist Negotiator (Buyer) Response:\\n{generalist_buyer_response}\\n\")\n",
    "del generalist_model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# load, generate response, and remove generalist model for seller prompt\n",
    "generalist_model = model_loader.reload_model(generalist_finetuned_dir)\n",
    "generalist_seller_response = generate_response(generalist_model, base_tokenizer, generalist_seller_prompt)\n",
    "print(f\"Generalist Negotiator (Seller) Prompt:\\n{generalist_seller_prompt}\\n\")\n",
    "print(f\"Generalist Negotiator (Seller) Response:\\n{generalist_seller_response}\\n\")\n",
    "del generalist_model\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
